{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dInfer Soft Token Experimentation Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Added dInfer to Python path: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python\n",
            "✓ Cleared cached dinfer modules\n",
            "✓ dInfer module found at: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python/dinfer/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Add dInfer to Python path from the 3rdparty submodule\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the dInfer python directory to sys.path\n",
        "DINFER_PATH = os.path.abspath('3rdparty/dInfer/python')\n",
        "if os.path.exists(DINFER_PATH):\n",
        "    if DINFER_PATH not in sys.path:\n",
        "        sys.path.insert(0, DINFER_PATH)\n",
        "    print(f\"✓ Added dInfer to Python path: {DINFER_PATH}\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: dInfer path not found: {DINFER_PATH}\")\n",
        "    print(\"  Make sure you're running this notebook from the project root.\")\n",
        "\n",
        "# Clear any cached imports to ensure we use the latest code\n",
        "import importlib\n",
        "for module_name in list(sys.modules.keys()):\n",
        "    if 'dinfer' in module_name:\n",
        "        del sys.modules[module_name]\n",
        "print(\"✓ Cleared cached dinfer modules\")\n",
        "\n",
        "# Verify the import works\n",
        "try:\n",
        "    import dinfer\n",
        "    print(f\"✓ dInfer module found at: {dinfer.__file__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import dInfer: {e}\")\n",
        "\n",
        "# Note: You can also use the import utilities from xp/llada_api/llada_generate/dinfer/_imports.py\n",
        "# which provides the same functionality with additional error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/nemo_rl_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-20 13:22:44 [__init__.py:216] Automatically detected platform cuda.\n",
            "Torch compilation disabled to avoid backend issues\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Disable torch compilation to avoid backend compiler errors\n",
        "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
        "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
        "\n",
        "# Disable torch.compile globally\n",
        "torch._dynamo.config.disable = True\n",
        "\n",
        "# dinfer imports (using local dInfer from 3rdparty/dInfer/python added to path above)\n",
        "from dinfer.model import LLaDAModelLM\n",
        "from dinfer.decoding.parallel_strategy import (\n",
        "    ParallelDecoder,\n",
        "    ThresholdParallelDecoder,\n",
        "    CreditThresholdParallelDecoder,\n",
        "    HierarchyDecoder,\n",
        "    get_num_transfer_tokens,\n",
        "    get_transfer_index,\n",
        ")\n",
        "from dinfer import (\n",
        "    BlockWiseDiffusionLLM,\n",
        "    BlockIteratorFactory,\n",
        "    KVCacheFactory,\n",
        "    SlidingWindowDiffusionLLM,\n",
        ")\n",
        "\n",
        "from dinfer.decoding.utils import (\n",
        "    TokenArray,\n",
        ")\n",
        "\n",
        "print(\"Torch compilation disabled to avoid backend issues\")\n",
        "\n",
        "# LLaDA tokenizer constants\n",
        "MASK_ID = 126336\n",
        "EOS_ID = 126081\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from GSAI-ML/LLaDA-8B-Instruct...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 6 files: 100%|██████████| 6/6 [00:12<00:00,  2.10s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 195.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "MODEL_PATH = \"GSAI-ML/LLaDA-8B-Instruct\"  # Update this path to your model\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "model = LLaDAModelLM.from_pretrained(\n",
        "    MODEL_PATH, \n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16, \n",
        "    init_device=str(device)  # Convert device to string for JSON serialization\n",
        ").eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: Compile the model for better performance\n",
        "# model = torch.compile(model, mode='reduce-overhead', fullgraph=True)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder and generation pipeline configured!\n"
          ]
        }
      ],
      "source": [
        "# Generation parameters - EXPERIMENT WITH THESE!\n",
        "generation_config = {\n",
        "    \"gen_length\": 256,      # Maximum number of tokens to generate\n",
        "    \"steps\": 64,\n",
        "    \"block_length\": 64,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": True,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "}\n",
        "\n",
        "# Create decoder with threshold strategy\n",
        "decoder = ThresholdParallelDecoder(0, threshold=generation_config[\"threshold\"], mask_id=MASK_ID, eos_id=EOS_ID)\n",
        "\n",
        "# Alternative: Use FixedParallelDecoderWithEOS for fixed-step decoding\n",
        "# decoder = FixedParallelDecoderWithEOS(\n",
        "#     temperature=0,\n",
        "#     steps=generation_config[\"steps\"],\n",
        "#     mask_id=MASK_ID,\n",
        "#     eos_id=EOS_ID\n",
        "# )\n",
        "# print(\"Using FixedParallelDecoderWithEOS\")\n",
        "\n",
        "# Create iterator factory\n",
        "iterator_factory = BlockIteratorFactory(True)\n",
        "\n",
        "# Create KV cache factory if using caching\n",
        "cache_factory = KVCacheFactory(generation_config[\"cache_type\"]) if generation_config[\"cache_type\"] else None\n",
        "\n",
        "# Create the Diffusion LLM instance\n",
        "dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        "    maximum_unroll=generation_config[\"maximum_unroll\"],\n",
        "    expected_tpf=generation_config[\"expected_tpf\"]\n",
        ")\n",
        "\n",
        "print(\"Decoder and generation pipeline configured!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt length: 8 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 236\n",
            "- Cache updates: 4\n",
            "- Time: 7.14s\n",
            "- Tokens/second: 35.74\n",
            "- Forwards/second: 33.08\n",
            "\n",
            "Generated text:\n",
            "Once upon a time in a magical forest, there lived a young girl named Lily. She was known for her kindness and bravery. One day, while exploring the forest, she stumbled upon a mysterious cave. As she entered the cave, she found a hidden room filled with sparkling crystals. The room was filled with a soft light, and Lily felt a strange sensation wash over her. Suddenly, she heard a gentle voice calling her name. She turned to see a wise old owl perched on a nearby branch. \"Welcome, Lily,\" the owl said. \"You have been chosen to be the guardian of the forest.\"\n",
            "\n",
            "Lily was amazed and scared. She had never been chosen for such a responsibility before. The owl explained that she had been chosen because of her kindness and bravery. She would have to protect the forest from any harm and make sure that the creatures that lived there were safe and happy. Lily knew that this was a big responsibility, but she was determined to do her best. She thanked the owl and promised to do everything in her power to protect the forest. From that day on, Lily became the guardian of the forest, and she was loved by all the creatures that lived there. She knew that she had found a special purpose, and she was proud to be a part of it.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(prompt, dllm_instance=dllm, tokenizer=tokenizer, config=generation_config):\n",
        "    \"\"\"\n",
        "    Generate text using the diffusion LLM.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        dllm_instance: Diffusion LLM instance\n",
        "        tokenizer: Tokenizer instance\n",
        "        config: Generation configuration dict\n",
        "    \n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "    \n",
        "    print(f\"Prompt length: {prompt_length} tokens\")\n",
        "    print(f\"Generating up to {config['gen_length']} tokens...\")\n",
        "    \n",
        "    # Track statistics\n",
        "    prev_forwards = dllm_instance.num_forwards\n",
        "    prev_cache_updates = dllm_instance.cache_updates\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    output_ids = dllm_instance.generate(\n",
        "        input_ids, \n",
        "        gen_length=config['gen_length'], \n",
        "        block_length=config['block_length']\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_forwards = dllm_instance.num_forwards - prev_forwards\n",
        "    total_cache_updates = dllm_instance.cache_updates - prev_cache_updates\n",
        "    generated_tokens = output_ids.shape[1] - prompt_length\n",
        "    generation_time = end_time - start_time\n",
        "    \n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nGeneration Statistics:\")\n",
        "    print(f\"- Generated tokens: {generated_tokens}\")\n",
        "    print(f\"- Forward passes: {total_forwards}\")\n",
        "    print(f\"- Cache updates: {total_cache_updates}\")\n",
        "    print(f\"- Time: {generation_time:.2f}s\")\n",
        "    print(f\"- Tokens/second: {generated_tokens/generation_time:.2f}\")\n",
        "    print(f\"- Forwards/second: {total_forwards/generation_time:.2f}\")\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "# Test the generation function\n",
        "test_prompt = \"Once upon a time in a magical forest\"\n",
        "generated = generate_text(test_prompt)\n",
        "print(f\"\\nGenerated text:\\n{generated}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT: The future of artificial intelligence is\n",
            "================================================================================\n",
            "Prompt length: 6 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 9\n",
            "- Forward passes: 36\n",
            "- Cache updates: 1\n",
            "- Time: 1.04s\n",
            "- Tokens/second: 8.64\n",
            "- Forwards/second: 34.57\n",
            "\n",
            "GENERATED:\n",
            "The future of artificial intelligence is bright, and the possibilities are endless.<|eot_id|>\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Interactive prompt examples\n",
        "prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Write a haiku about programming:\",\n",
        "    \"The most important scientific discovery was\",\n",
        "]\n",
        "\n",
        "# Generate for each prompt\n",
        "for prompt in prompts[:1]:  # Change to prompts to run all\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    generated = generate_text(prompt)\n",
        "    print(f\"\\nGENERATED:\\n{generated}\")\n",
        "    print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.7\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 100\n",
            "- Cache updates: 3\n",
            "- Time: 2.89s\n",
            "- Tokens/second: 45.05\n",
            "- Forwards/second: 34.65\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.8\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 113\n",
            "- Cache updates: 3\n",
            "- Time: 3.26s\n",
            "- Tokens/second: 39.91\n",
            "- Forwards/second: 34.69\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.9\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 122\n",
            "- Cache updates: 3\n",
            "- Time: 3.51s\n",
            "- Tokens/second: 36.99\n",
            "- Forwards/second: 34.72\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.95\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 130\n",
            "- Cache updates: 3\n",
            "- Time: 3.75s\n",
            "- Tokens/second: 34.69\n",
            "- Forwards/second: 34.69\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different threshold values\n",
        "thresholds = [0.7, 0.8, 0.9, 0.95]\n",
        "test_prompt = \"The meaning of life is\"\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing with threshold = {threshold}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create new decoder with different threshold\n",
        "    test_decoder = ThresholdParallelDecoder(0, threshold=threshold)\n",
        "    \n",
        "    # Create new DLLM instance\n",
        "    test_dllm = BlockWiseDiffusionLLM(\n",
        "        model=model,\n",
        "        decoder=test_decoder,\n",
        "        iterator_factory=iterator_factory,\n",
        "        cache_factory=cache_factory,\n",
        "        early_stop=True\n",
        "    )\n",
        "    \n",
        "    # Generate and compare\n",
        "    generated = generate_text(test_prompt, dllm_instance=test_dllm)\n",
        "    print(f\"\\nGenerated: {generated[len(test_prompt):]}\")  # Show only generated part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block-wise generation:\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 70\n",
            "- Forward passes: 83\n",
            "- Cache updates: 2\n",
            "- Time: 2.40s\n",
            "- Tokens/second: 29.12\n",
            "- Forwards/second: 34.53\n",
            "\n",
            "\n",
            "Sliding window generation:\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 67\n",
            "- Forward passes: 74\n",
            "- Cache updates: 2\n",
            "- Time: 2.16s\n",
            "- Tokens/second: 31.07\n",
            "- Forwards/second: 34.32\n",
            "\n",
            "\n",
            "Comparison:\n",
            "Block-wise output:  the way we live, work, and interact with each other. It will enable us to solve complex problems, i...\n",
            "Sliding window output:  the way we live, work, and interact with each other. It will enable us to be more efficient, produc...\n"
          ]
        }
      ],
      "source": [
        "# Create sliding window DLLM\n",
        "sliding_dllm = SlidingWindowDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=KVCacheFactory('dual'),  # Sliding window requires cache\n",
        "    prefix_look=0,      # How many tokens to look back\n",
        "    after_look=0,       # How many tokens to look ahead\n",
        "    warmup_steps=1,     # Warmup iterations\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Compare block-wise vs sliding window\n",
        "test_prompt = \"Artificial intelligence will revolutionize\"\n",
        "\n",
        "print(\"Block-wise generation:\")\n",
        "print(\"=\"*80)\n",
        "block_generated = generate_text(test_prompt, dllm_instance=dllm)\n",
        "\n",
        "print(\"\\n\\nSliding window generation:\")\n",
        "print(\"=\"*80)\n",
        "sliding_generated = generate_text(test_prompt, dllm_instance=sliding_dllm)\n",
        "\n",
        "print(\"\\n\\nComparison:\")\n",
        "print(f\"Block-wise output: {block_generated[len(test_prompt):][:100]}...\")\n",
        "print(f\"Sliding window output: {sliding_generated[len(test_prompt):][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _FixedParallelDecoder(ParallelDecoder):\n",
        "    \"\"\" This decoder decodes tokens in a fixed number of steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature, steps, remasking='low_confidence', mask_id=MASK_ID, eos_id=EOS_ID):\n",
        "        super().__init__(temperature, remasking, mask_id)\n",
        "        self.steps = steps\n",
        "        self.iter = 0\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def block_init(self, block_x, block_id):\n",
        "        # TODO(zhengda) we need to handle steps correctly here when the distributed version changes the gen length.\n",
        "        block_mask_index = block_x == self.mask_id\n",
        "        self.num_transfer_tokens = get_num_transfer_tokens(block_mask_index, self.steps)\n",
        "        self.iter = 0\n",
        "\n",
        "    def decode(self, logits, block_start, block_end, x, iter_threshold = None):\n",
        "        \"\"\" Decode the logits in a block.\n",
        "        \"\"\"\n",
        "        mask_index = (x[block_start:block_end] == self.mask_id)\n",
        "        assert mask_index.shape[1] == logits.shape[1]\n",
        "\n",
        "        curr_x = x[block_start:block_end]\n",
        "        x0, transfer_index = get_transfer_index(logits, self.temperature, self.remasking, mask_index, curr_x, self.num_transfer_tokens[:, self.iter], None)\n",
        "        self.iter += 1\n",
        "        x[block_start:block_end][transfer_index] = x0[transfer_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FixedParallelDecoder with different fixed ratios...\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 18\n",
            "- Forward passes: 64\n",
            "- Cache updates: 1\n",
            "- Time: 1.93s\n",
            "- Tokens/second: 9.32\n",
            "- Forwards/second: 33.13\n",
            "\n",
            "Generated:  to be open to new ideas and to be willing to take risks. It's not about...\n"
          ]
        }
      ],
      "source": [
        "# Experiment with FixedParallelDecoder if available\n",
        "print(\"Testing FixedParallelDecoder with different fixed ratios...\")\n",
        "\n",
        "test_prompt = \"The key to innovation is\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "# Create DLLM with fixed decoder\n",
        "fixed_dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Generate\n",
        "generated = generate_text(test_prompt, dllm_instance=fixed_dllm)\n",
        "print(f\"\\nGenerated: {generated[len(test_prompt):][:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft Token Diffusion Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlockWiseSoftTokenLLM:\n",
        "    def __init__(self, model, decoder, iterator_factory, early_stop=True, cache_factory=None, maximum_unroll=4, expected_tpf=8, soft_token_ratio=0.2):\n",
        "        self.model = model\n",
        "        self.cache_factory = cache_factory\n",
        "        self.decoder = decoder\n",
        "        self.iterator_factory = iterator_factory\n",
        "        self.num_forwards = 0\n",
        "        self.cache_updates = 0\n",
        "        self.early_stop = early_stop\n",
        "        self.maximum_unroll = maximum_unroll\n",
        "        self.expected_tpf = expected_tpf\n",
        "        self.soft_token_ratio = soft_token_ratio\n",
        "        self.input_embeddings = self.model.get_input_embeddings()\n",
        "        \n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, gen_length=128, block_length=128):\n",
        "        ''' Generate tokens with diffusion iterations block by block using Soft Token Sampling.\n",
        "        '''\n",
        "        x = TokenArray(prompt, gen_length, self.decoder.mask_id, self.decoder.eos_id, self.model.device)\n",
        "        it = self.iterator_factory.create(x, block_length)\n",
        "\n",
        "        iter_no = 0\n",
        "        kv_cache = self.cache_factory.create() if self.cache_factory is not None else None\n",
        "        \n",
        "        for block_id, (block_loc, block) in enumerate(it):\n",
        "            self.decoder.block_init(block, block_id)\n",
        "\n",
        "            while (block == self.decoder.mask_id).sum() > 0:\n",
        "                \n",
        "                # Helper to run model with correct context and embeddings\n",
        "                def run_model(use_input_embeds=None):\n",
        "                    # Determine input context based on cache type\n",
        "                    if kv_cache is None:\n",
        "                        # Full context (no cache)\n",
        "                        if use_input_embeds is not None:\n",
        "                            logits = self.model(inputs_embeds=use_input_embeds).logits\n",
        "                        else:\n",
        "                            logits = self.model(x.data).logits\n",
        "                        return logits[:, block_loc.start:block_loc.end]\n",
        "                        \n",
        "                    elif kv_cache.cache_type == 'prefix':\n",
        "                        # Prefix Cache: past_key_values contains context up to block_start\n",
        "                        past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "                        \n",
        "                        if use_input_embeds is not None:\n",
        "                            # Input embeddings should correspond to x[block_loc.start:]\n",
        "                            logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        else:\n",
        "                            logits = self.model(x[block_loc.start:], past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        \n",
        "                        curr_len = block_loc.end - block_loc.start\n",
        "                        return logits[:, :curr_len]\n",
        "\n",
        "                    else:\n",
        "                        # Dual/Sliding Cache: typically uses block context\n",
        "                        past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "                        \n",
        "                        if use_input_embeds is not None:\n",
        "                             logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        else:\n",
        "                             logits = self.model(block, past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        return logits\n",
        "\n",
        "                # 1. Handle KV Cache Update (Initial step for block or periodically)\n",
        "                if kv_cache is not None and kv_cache.require_update(iter_no, block_loc.start, block_loc.end):\n",
        "                    output = self.model(x.data, use_cache=True)\n",
        "                    self.num_forwards += 1\n",
        "                    \n",
        "                    # Update cache\n",
        "                    kv_cache.update(output.past_key_values)\n",
        "                    self.cache_updates += 1\n",
        "                    \n",
        "                    # Decode using these initial logits\n",
        "                    self.decoder.decode(output.logits[:, block_loc.start:block_loc.end], block_loc.start, block_loc.end, x)\n",
        "                    iter_no += 1\n",
        "                    continue\n",
        "\n",
        "                # 2. Pass 1: Standard Logits (with current masks)\n",
        "                logits1 = run_model(use_input_embeds=None)\n",
        "                self.num_forwards += 1\n",
        "                \n",
        "                decoding_logits = logits1\n",
        "                \n",
        "                # 3. Soft Token Logic\n",
        "                # Identify masks in the current block\n",
        "                curr_block_ids = x[block_loc.start:block_loc.end]\n",
        "                mask_mask = (curr_block_ids == self.decoder.mask_id)\n",
        "                mask_indices = torch.nonzero(mask_mask).flatten() # Indices relative to block start\n",
        "                \n",
        "                if mask_indices.numel() > 0 and self.soft_token_ratio > 0:\n",
        "                    # Sample masks to turn into soft tokens\n",
        "                    num_soft = int(mask_indices.numel() * self.soft_token_ratio)\n",
        "                    if num_soft > 0:\n",
        "                        perm = torch.randperm(mask_indices.numel(), device=self.model.device)\n",
        "                        soft_indices = mask_indices[perm[:num_soft]] # Indices relative to block start\n",
        "                        \n",
        "                        # Extract logits for these positions\n",
        "                        # logits1 shape: [1, block_len, vocab]\n",
        "                        selected_logits = logits1[0, soft_indices]\n",
        "                        probs = torch.softmax(selected_logits, dim=-1)\n",
        "                        \n",
        "                        # Compute Soft Embeddings: Weighted average of token embeddings\n",
        "                        # [num_soft, vocab] @ [vocab, d_model] -> [num_soft, d_model]\n",
        "                        soft_embeds = torch.matmul(probs, self.input_embeddings.weight)\n",
        "                        \n",
        "                        # Prepare Input Embeddings\n",
        "                        target_ids = None\n",
        "                        global_offset = 0\n",
        "                        \n",
        "                        if kv_cache is None:\n",
        "                            target_ids = x.data\n",
        "                            global_offset = block_loc.start # Offset in target_ids\n",
        "                        elif kv_cache.cache_type == 'prefix':\n",
        "                            target_ids = x[block_loc.start:]\n",
        "                            global_offset = 0 # relative to start of target_ids\n",
        "                        else:\n",
        "                            target_ids = curr_block_ids\n",
        "                            global_offset = 0\n",
        "                        \n",
        "                        # Get base embeddings for the input context\n",
        "                        # We clone to avoid modifying the model's internal cache if any (unlikely here)\n",
        "                        inputs_embeds = self.input_embeddings(target_ids).clone() # [1, len, d_model]\n",
        "                        \n",
        "                        # Replace masks with soft embeddings\n",
        "                        inputs_embeds[0, global_offset + soft_indices] = soft_embeds\n",
        "                        \n",
        "                        # Pass 2: Get logits with Soft Tokens\n",
        "                        logits2 = run_model(use_input_embeds=inputs_embeds)\n",
        "                        self.num_forwards += 1\n",
        "                        decoding_logits = logits2\n",
        "\n",
        "                # 4. Decode using the latest logits\n",
        "                self.decoder.decode(decoding_logits, block_loc.start, block_loc.end, x)\n",
        "                iter_no += 1\n",
        "\n",
        "            # Early stop at EOS\n",
        "            if self.early_stop and torch.any(x[block_loc.start:block_loc.end] == self.decoder.eos_id):\n",
        "                x[block_loc.end:] = self.decoder.eos_id\n",
        "                break\n",
        "\n",
        "        return x.get_generated_tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating with Soft Token LLM...\n",
            "Prompt length: 7 tokens\n",
            "Generating up to 1024 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 140\n",
            "- Forward passes: 128\n",
            "- Cache updates: 2\n",
            "- Time: 8.72s\n",
            "- Tokens/second: 16.05\n",
            "- Forwards/second: 14.68\n",
            "\n",
            "Generated: \n",
            "\n",
            "The theory of relativity is proposed by Albert Einstein. It is a  two-part theory: Special\n",
            "Relativity and General Relativity.\n",
            "\n",
            "Special Relativity:\n",
            "\n",
            "1. The speed of light is always constant and is independent of the observer's motion.\n",
            "2. The laws of physics are the same for all observers in uniform motion.\n",
            "3. Time and relative motion are inter.\n",
            "4. Length and time time change for an object in motion relative to an observer.\n",
            "\n",
            "\n",
            "General Relativity:\n",
            "\n",
            "1. Gravity is not a force but rather a curvature of spacetime caused by mass and energy.\n",
            "2. Objects move along the curvature of spacetime.<|eot_id|>...\n",
            "--------------------------------\n",
            "Generating with Block Wise LLM...\n",
            "Prompt length: 7 tokens\n",
            "Generating up to 1024 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 42\n",
            "- Forward passes: 16\n",
            "- Cache updates: 1\n",
            "- Time: 1.10s\n",
            "- Tokens/second: 38.21\n",
            "- Forwards/second: 14.56\n",
            "\n",
            "Generated: \n",
            "\n",
            "The theory of relativity is a by Albert Einstein Albert Einstein. It theory of relativity is a theory of physics that of the the the the the the theory of the and of relativity.\n",
            "\n",
            "\n",
            "\n",
            "...\n"
          ]
        }
      ],
      "source": [
        "generation_config = {\n",
        "    \"gen_length\": 1024,      # Maximum number of tokens to generate\n",
        "    \"steps\": 16,\n",
        "    \"block_length\": 128,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": True,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "}\n",
        "\n",
        "test_prompt = \"About the theory of relativity: \"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "soft_llm = BlockWiseSoftTokenLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True,\n",
        "    soft_token_ratio=0.5,\n",
        ")\n",
        "\n",
        "block_wise_llm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True,\n",
        ")\n",
        "\n",
        "# Generate\n",
        "print(\"Generating with Soft Token LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=fixed_dllm, config=generation_config)\n",
        "print(f\"\\nGenerated: {generated[len(test_prompt):]}...\")\n",
        "\n",
        "print(\"--------------------------------\")\n",
        "\n",
        "print(\"Generating with Block Wise LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=block_wise_llm, config=generation_config)\n",
        "print(f\"\\nGenerated: {generated[len(test_prompt):]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLURM Job Kernel (root)",
      "language": "python",
      "name": "slurm-job-kernel-mfathi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
