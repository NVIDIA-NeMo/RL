{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dInfer Soft Token Experimentation Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Added dInfer to Python path: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python\n",
            "✓ Cleared cached dinfer modules\n",
            "✓ dInfer module found at: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/xp/llada_api/llada_generate/dinfer/../../../../3rdparty/dInfer/python/dinfer/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Add dInfer to Python path from the 3rdparty submodule\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the dInfer python directory to sys.path\n",
        "DINFER_PATH = os.path.abspath('3rdparty/dInfer/python')\n",
        "if os.path.exists(DINFER_PATH):\n",
        "    if DINFER_PATH not in sys.path:\n",
        "        sys.path.insert(0, DINFER_PATH)\n",
        "    print(f\"✓ Added dInfer to Python path: {DINFER_PATH}\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: dInfer path not found: {DINFER_PATH}\")\n",
        "    print(\"  Make sure you're running this notebook from the project root.\")\n",
        "\n",
        "# Clear any cached imports to ensure we use the latest code\n",
        "import importlib\n",
        "for module_name in list(sys.modules.keys()):\n",
        "    if 'dinfer' in module_name:\n",
        "        del sys.modules[module_name]\n",
        "print(\"✓ Cleared cached dinfer modules\")\n",
        "\n",
        "# Verify the import works\n",
        "try:\n",
        "    import dinfer\n",
        "    print(f\"✓ dInfer module found at: {dinfer.__file__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import dInfer: {e}\")\n",
        "\n",
        "# Note: You can also use the import utilities from xp/llada_api/llada_generate/dinfer/_imports.py\n",
        "# which provides the same functionality with additional error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch compilation disabled to avoid backend issues\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Disable torch compilation to avoid backend compiler errors\n",
        "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
        "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
        "\n",
        "# Disable torch.compile globally\n",
        "torch._dynamo.config.disable = True\n",
        "\n",
        "# dinfer imports (using local dInfer from 3rdparty/dInfer/python added to path above)\n",
        "from dinfer.model import LLaDAModelLM\n",
        "from dinfer.decoding.parallel_strategy import (\n",
        "    ParallelDecoder,\n",
        "    ThresholdParallelDecoder,\n",
        "    CreditThresholdParallelDecoder,\n",
        "    HierarchyDecoder,\n",
        "    get_num_transfer_tokens,\n",
        "    get_transfer_index,\n",
        ")\n",
        "from dinfer import (\n",
        "    BlockWiseDiffusionLLM,\n",
        "    BlockIteratorFactory,\n",
        "    KVCacheFactory,\n",
        "    SlidingWindowDiffusionLLM,\n",
        ")\n",
        "\n",
        "from dinfer.decoding.utils import (\n",
        "    TokenArray,\n",
        ")\n",
        "\n",
        "print(\"Torch compilation disabled to avoid backend issues\")\n",
        "\n",
        "# LLaDA tokenizer constants\n",
        "MASK_ID = 126336\n",
        "EOS_ID = 126081\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from GSAI-ML/LLaDA-1.5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 6 files: 100%|██████████| 6/6 [00:27<00:00,  4.63s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 311.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "# MODEL_PATH = \"GSAI-ML/LLaDA-8B-Instruct\"  # Update this path to your model\n",
        "MODEL_PATH = \"GSAI-ML/LLaDA-1.5\"\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "model = LLaDAModelLM.from_pretrained(\n",
        "    MODEL_PATH, \n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16, \n",
        "    init_device=str(device)  # Convert device to string for JSON serialization\n",
        ").eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: Compile the model for better performance\n",
        "# model = torch.compile(model, mode='reduce-overhead', fullgraph=True)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder and generation pipeline configured!\n"
          ]
        }
      ],
      "source": [
        "# Generation parameters - EXPERIMENT WITH THESE!\n",
        "generation_config = {\n",
        "    \"gen_length\": 256,      # Maximum number of tokens to generate\n",
        "    \"steps\": 64,\n",
        "    \"block_length\": 64,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": True,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "}\n",
        "\n",
        "# Create decoder with threshold strategy\n",
        "decoder = ThresholdParallelDecoder(0, threshold=generation_config[\"threshold\"], mask_id=MASK_ID, eos_id=EOS_ID)\n",
        "\n",
        "# Alternative: Use FixedParallelDecoderWithEOS for fixed-step decoding\n",
        "# decoder = FixedParallelDecoderWithEOS(\n",
        "#     temperature=0,\n",
        "#     steps=generation_config[\"steps\"],\n",
        "#     mask_id=MASK_ID,\n",
        "#     eos_id=EOS_ID\n",
        "# )\n",
        "# print(\"Using FixedParallelDecoderWithEOS\")\n",
        "\n",
        "# Create iterator factory\n",
        "iterator_factory = BlockIteratorFactory(True)\n",
        "\n",
        "# Create KV cache factory if using caching\n",
        "cache_factory = KVCacheFactory(generation_config[\"cache_type\"]) if generation_config[\"cache_type\"] else None\n",
        "\n",
        "# Create the Diffusion LLM instance\n",
        "dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        "    maximum_unroll=generation_config[\"maximum_unroll\"],\n",
        "    expected_tpf=generation_config[\"expected_tpf\"]\n",
        ")\n",
        "\n",
        "print(\"Decoder and generation pipeline configured!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt length: 21 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 220\n",
            "- Cache updates: 4\n",
            "- Time: 4.46s\n",
            "- Tokens/second: 57.18\n",
            "- Forwards/second: 49.33\n",
            "\n",
            "Generated text:\n",
            "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Once upon a time in a magical forest<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Once upon a time in a magical forest, where the trees whispered secrets and the streams sang lullabies, there lived a young girl named Elara. Elara was known throughout the forest for her kindness and her adventurous spirit. She had a heart as pure as the morning dew and a spirit as free as the summer breeze.\n",
            "\n",
            "One day, while exploring the forest, Elara stumbled upon a hidden glade bathed in the soft glow of twilight. The glade was home to a variety of enchanting creatures, from glowing butterflies to wise old owls. As Elara wandered deeper into the glade, she came across a small, shimmering pond. The water seemed to shimmer with an otherworldly light, and the air was filled with the sweet scent of blooming flowers.\n",
            "\n",
            "Elara approached the pond with reverence, sensing that it held great magic. As she dipped her fingers into the water, she felt a surge of energy course through her. Suddenly, a soft, melodic voice echoed through the glade, \"Elara, you have been chosen to protect the secrets of the forest.\"\n",
            "\n",
            "From that day forward, Elara was known as the guardian of the magical forest, and her story was told for generations to come.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(prompt, dllm_instance=dllm, tokenizer=tokenizer, config=generation_config):\n",
        "    \"\"\"\n",
        "    Generate text using the diffusion LLM.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        dllm_instance: Diffusion LLM instance\n",
        "        tokenizer: Tokenizer instance\n",
        "        config: Generation configuration dict\n",
        "    \n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    message = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False)\n",
        "    input_ids = tokenizer(message, return_tensors=\"pt\")['input_ids'].to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "    \n",
        "    print(f\"Prompt length: {prompt_length} tokens\")\n",
        "    print(f\"Generating up to {config['gen_length']} tokens...\")\n",
        "    \n",
        "    # Track statistics\n",
        "    prev_forwards = dllm_instance.num_forwards\n",
        "    prev_cache_updates = dllm_instance.cache_updates\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    output_ids = dllm_instance.generate(\n",
        "        input_ids, \n",
        "        gen_length=config['gen_length'], \n",
        "        block_length=config['block_length']\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_forwards = dllm_instance.num_forwards - prev_forwards\n",
        "    total_cache_updates = dllm_instance.cache_updates - prev_cache_updates\n",
        "    generated_tokens = output_ids.shape[1] - prompt_length\n",
        "    generation_time = end_time - start_time\n",
        "    \n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nGeneration Statistics:\")\n",
        "    print(f\"- Generated tokens: {generated_tokens}\")\n",
        "    print(f\"- Forward passes: {total_forwards}\")\n",
        "    print(f\"- Cache updates: {total_cache_updates}\")\n",
        "    print(f\"- Time: {generation_time:.2f}s\")\n",
        "    print(f\"- Tokens/second: {generated_tokens/generation_time:.2f}\")\n",
        "    print(f\"- Forwards/second: {total_forwards/generation_time:.2f}\")\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "# Test the generation function\n",
        "test_prompt = \"Once upon a time in a magical forest\"\n",
        "generated = generate_text(test_prompt)\n",
        "print(f\"\\nGenerated text:\\n{generated}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT: The future of artificial intelligence is\n",
            "================================================================================\n",
            "Prompt length: 19 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 71\n",
            "- Forward passes: 91\n",
            "- Cache updates: 2\n",
            "- Time: 1.75s\n",
            "- Tokens/second: 40.59\n",
            "- Forwards/second: 52.03\n",
            "\n",
            "GENERATED:\n",
            "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "The future of artificial intelligence is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The future of artificial intelligence is bright and promising. As technology continues to advance, AI is expected to revolutionize various aspects of our lives, from healthcare to education, transportation to entertainment. AI will enable us to solve complex problems, improve efficiency, and create new opportunities. However, it also comes with challenges, such as ethical considerations and job displacement.<|eot_id|>\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Interactive prompt examples\n",
        "prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Write a haiku about programming:\",\n",
        "    \"The most important scientific discovery was\",\n",
        "]\n",
        "\n",
        "# Generate for each prompt\n",
        "for prompt in prompts[:1]:  # Change to prompts to run all\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    generated = generate_text(prompt)\n",
        "    print(f\"\\nGENERATED:\\n{generated}\")\n",
        "    print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.7\n",
            "================================================================================\n",
            "Prompt length: 18 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 196\n",
            "- Cache updates: 4\n",
            "- Time: 3.76s\n",
            "- Tokens/second: 67.89\n",
            "- Forwards/second: 52.18\n",
            "\n",
            "Generated: _header_id|>user<|end_header_id|>\n",
            "\n",
            "The meaning of life is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The meaning of life is a profound and complex question that has been debated by philosophers, scientists, and thinkers throughout history. It is a deeply personal and subjective matter, and different people may have different answers. Some believe that the meaning of life is to find happiness, love, and fulfillment, while others may see it as a chance to contribute to the greater good or to explore the mysteries of the universe.\n",
            "\n",
            "From a scientific perspective, the meaning of life is often discussed in terms of evolution, survival, and the continuation of the human species. From a philosophical standpoint, it might involve exploring the nature of existence, consciousness, and the universe.\n",
            "\n",
            "Many religious and spiritual traditions offer perspectives on the meaning of life, ranging from the worship of a higher power to the pursuit of enlightenment or salvation. Others may seek secular answers, such as the pursuit of knowledge, love, and personal growth.\n",
            "\n",
            "Ultimately, the meaning of life is likely something that each person must discover for themselves, through experience, reflection, and exploration. It is a question that may not have a definitive answer, but rather a journey of self-discovery and growth. Regardless of what one finds, the pursuit of a meaning in life can provide a sense of purpose, direction, and fulfillment.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.8\n",
            "================================================================================\n",
            "Prompt length: 18 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 221\n",
            "- Cache updates: 4\n",
            "- Time: 4.24s\n",
            "- Tokens/second: 60.18\n",
            "- Forwards/second: 52.16\n",
            "\n",
            "Generated: _header_id|>user<|end_header_id|>\n",
            "\n",
            "The meaning of life is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The meaning of life is a profound and complex question that has puzzled philosophers, theologians, and thinkers for centuries. There is no single, universally accepted answer, as it varies from person to person. Some believe that the meaning of life is to find happiness, fulfillment, or purpose, while others see it as a journey of self-discovery, growth, or enlightenment.\n",
            "\n",
            "From a spiritual perspective, the meaning of life might be to connect with a higher power or to achieve enlightenment. From a philosophical standpoint, it could be to contribute to the betterment of society or to find personal fulfillment.\n",
            "\n",
            "In many cultures, the meaning of life is often tied to the pursuit of knowledge, love, and happiness. However, the true meaning of life is likely a combination of all these factors, along with many others.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal and subjective experience. It is something that each individual must discover for themselves, often through introspection, exploration, and reflection.\n",
            "\n",
            "If you're looking for a more concrete answer, you might want to consider what truly matters to you in life. Is it happiness, success, love, or something else entirely? Reflecting on these questions can help you find your own unique meaning in life.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.9\n",
            "================================================================================\n",
            "Prompt length: 18 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 236\n",
            "- Cache updates: 4\n",
            "- Time: 4.52s\n",
            "- Tokens/second: 56.36\n",
            "- Forwards/second: 52.16\n",
            "\n",
            "Generated: _header_id|>user<|end_header_id|>\n",
            "\n",
            "The meaning of life is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The meaning of life is a profound and complex question that has puzzled philosophers, theologians, and thinkers for centuries. There is no single, universally accepted answer, as it varies from person to person. Some believe that the meaning of life is to find happiness, fulfillment, or purpose, while others see it as a journey of self-discovery, growth, or enlightenment.\n",
            "\n",
            "From a spiritual perspective, the meaning of life might be to connect with a higher power or to achieve enlightenment. From a philosophical standpoint, it could be to contribute to the betterment of society or to find personal fulfillment.\n",
            "\n",
            "In many cultures, the meaning of life is often tied to the pursuit of knowledge, love, and happiness. However, the true meaning of life is likely a combination of all these factors, along with many others.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal and subjective experience. It is something that each individual must discover for themselves, often through introspection, exploration, and reflection.\n",
            "\n",
            "If you're looking for a more concrete answer, you might want to consider what truly matters to you in life. Is it happiness, success, love, or something else entirely? Reflecting on these questions can help you find your own unique meaning in life.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.95\n",
            "================================================================================\n",
            "Prompt length: 18 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 242\n",
            "- Cache updates: 4\n",
            "- Time: 4.64s\n",
            "- Tokens/second: 54.95\n",
            "- Forwards/second: 52.15\n",
            "\n",
            "Generated: _header_id|>user<|end_header_id|>\n",
            "\n",
            "The meaning of life is<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The meaning of life is a profound and complex question that has puzzled philosophers, theologians, and thinkers for centuries. There is no single, universally accepted answer, as it varies from person to person. Some believe that the meaning of life is to find happiness, fulfillment, or purpose, while others see it as a journey of self-discovery, growth, or enlightenment.\n",
            "\n",
            "From a spiritual perspective, the meaning of life might be to connect with a higher power or to achieve enlightenment. From a philosophical standpoint, it could be to contribute to the betterment of society or to find personal fulfillment.\n",
            "\n",
            "In many cultures, the meaning of life is often tied to the pursuit of knowledge, love, and happiness. However, the true meaning of life is likely a combination of all these factors, along with many others.\n",
            "\n",
            "Ultimately, the meaning of life is a deeply personal and subjective experience. It is something that each individual must discover for themselves, often through introspection, exploration, and reflection.\n",
            "\n",
            "If you're looking for a more concrete answer, you might want to consider what truly matters to you in life. Is it happiness, success, love, or something else entirely? Reflecting on these questions can help you find your own unique meaning in life.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different threshold values\n",
        "thresholds = [0.7, 0.8, 0.9, 0.95]\n",
        "test_prompt = \"The meaning of life is\"\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing with threshold = {threshold}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create new decoder with different threshold\n",
        "    test_decoder = ThresholdParallelDecoder(0, threshold=threshold)\n",
        "    \n",
        "    # Create new DLLM instance\n",
        "    test_dllm = BlockWiseDiffusionLLM(\n",
        "        model=model,\n",
        "        decoder=test_decoder,\n",
        "        iterator_factory=iterator_factory,\n",
        "        cache_factory=cache_factory,\n",
        "        early_stop=True\n",
        "    )\n",
        "    \n",
        "    # Generate and compare\n",
        "    generated = generate_text(test_prompt, dllm_instance=test_dllm)\n",
        "    print(f\"\\nGenerated: {generated[len(test_prompt):]}\")  # Show only generated part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block-wise generation:\n",
            "================================================================================\n",
            "Prompt length: 18 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 203\n",
            "- Cache updates: 4\n",
            "- Time: 3.89s\n",
            "- Tokens/second: 65.75\n",
            "- Forwards/second: 52.14\n",
            "\n",
            "\n",
            "Sliding window generation:\n",
            "================================================================================\n",
            "Prompt length: 18 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 203\n",
            "- Cache updates: 4\n",
            "- Time: 3.89s\n",
            "- Tokens/second: 65.81\n",
            "- Forwards/second: 52.18\n",
            "\n",
            "\n",
            "Comparison:\n",
            "Block-wise output: d_header_id|>\n",
            "\n",
            "Artificial intelligence will revolutionize<|eot_id|><|start_header_id|>assistant<|end...\n",
            "Sliding window output: d_header_id|>\n",
            "\n",
            "Artificial intelligence will revolutionize<|eot_id|><|start_header_id|>assistant<|end...\n"
          ]
        }
      ],
      "source": [
        "# Create sliding window DLLM\n",
        "sliding_dllm = SlidingWindowDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=KVCacheFactory('dual'),  # Sliding window requires cache\n",
        "    prefix_look=0,      # How many tokens to look back\n",
        "    after_look=0,       # How many tokens to look ahead\n",
        "    warmup_steps=1,     # Warmup iterations\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Compare block-wise vs sliding window\n",
        "test_prompt = \"Artificial intelligence will revolutionize\"\n",
        "\n",
        "print(\"Block-wise generation:\")\n",
        "print(\"=\"*80)\n",
        "block_generated = generate_text(test_prompt, dllm_instance=dllm)\n",
        "\n",
        "print(\"\\n\\nSliding window generation:\")\n",
        "print(\"=\"*80)\n",
        "sliding_generated = generate_text(test_prompt, dllm_instance=sliding_dllm)\n",
        "\n",
        "print(\"\\n\\nComparison:\")\n",
        "print(f\"Block-wise output: {block_generated[len(test_prompt):][:100]}...\")\n",
        "print(f\"Sliding window output: {sliding_generated[len(test_prompt):][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _FixedParallelDecoder(ParallelDecoder):\n",
        "    \"\"\" This decoder decodes tokens in a fixed number of steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature, steps, remasking='low_confidence', mask_id=MASK_ID, eos_id=EOS_ID):\n",
        "        super().__init__(temperature, remasking, mask_id)\n",
        "        self.steps = steps\n",
        "        self.iter = 0\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def block_init(self, block_x, block_id):\n",
        "        # TODO(zhengda) we need to handle steps correctly here when the distributed version changes the gen length.\n",
        "        block_mask_index = block_x == self.mask_id\n",
        "        self.num_transfer_tokens = get_num_transfer_tokens(block_mask_index, self.steps)\n",
        "        self.iter = 0\n",
        "\n",
        "    def decode(self, logits, block_start, block_end, x, iter_threshold = None):\n",
        "        \"\"\" Decode the logits in a block.\n",
        "        \"\"\"\n",
        "        mask_index = (x[block_start:block_end] == self.mask_id)\n",
        "        assert mask_index.shape[1] == logits.shape[1]\n",
        "\n",
        "        curr_x = x[block_start:block_end]\n",
        "        x0, transfer_index = get_transfer_index(logits, self.temperature, self.remasking, mask_index, curr_x, self.num_transfer_tokens[:, self.iter], None)\n",
        "        self.iter += 1\n",
        "        x[block_start:block_end][transfer_index] = x0[transfer_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FixedParallelDecoder with different fixed ratios...\n",
            "Prompt length: 20 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 57\n",
            "- Forward passes: 64\n",
            "- Cache updates: 1\n",
            "- Time: 1.26s\n",
            "- Tokens/second: 45.25\n",
            "- Forwards/second: 50.81\n",
            "\n",
            "Generated: id|>user<|end_header_id|>\n",
            "\n",
            "What is the key to innovation?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The key to innovation is a combination of creativity, curiosity, and a willingness to take risks. It involves challenging the status quo, exploring new ideas, and being open to feedback and iteration. Additionally, fostering a culture of collaboration, experimentation, and continuous learning can help drive innovation.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Experiment with FixedParallelDecoder if available\n",
        "print(\"Testing FixedParallelDecoder with different fixed ratios...\")\n",
        "\n",
        "test_prompt = \"What is the key to innovation?\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "# Create DLLM with fixed decoder\n",
        "fixed_dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Generate\n",
        "generated = generate_text(test_prompt, dllm_instance=fixed_dllm)\n",
        "print(f\"\\nGenerated: {generated[len(test_prompt):]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft Token Diffusion Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlockWiseSoftTokenLLM:\n",
        "    \"\"\"\n",
        "    Block-wise diffusion LLM with Soft Token Sampling.\n",
        "    Adapted from BlockWiseSoftTokenLLM in soft_token_experiment.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, decoder, iterator_factory, early_stop=True, cache_factory=None, maximum_unroll=4, expected_tpf=8, soft_token_ratio=0.2, treat_soft_tokens_as_candidates=False, soft_temperature=1.0):\n",
        "        self.model = model\n",
        "        self.cache_factory = cache_factory\n",
        "        self.decoder = decoder\n",
        "        self.iterator_factory = iterator_factory\n",
        "        self.num_forwards = 0\n",
        "        self.cache_updates = 0\n",
        "        self.early_stop = early_stop\n",
        "        self.maximum_unroll = maximum_unroll\n",
        "        self.expected_tpf = expected_tpf\n",
        "        self.soft_token_ratio = soft_token_ratio\n",
        "        self.treat_soft_tokens_as_candidates = treat_soft_tokens_as_candidates\n",
        "        self.soft_temperature = soft_temperature\n",
        "        self.input_embeddings = self.model.get_input_embeddings()\n",
        "\n",
        "    def _compute_logits(self, x, block_loc, kv_cache, use_input_embeds=None):\n",
        "        \"\"\"Helper to run model with correct context and embeddings.\"\"\"\n",
        "        # Determine input context based on cache type\n",
        "        if kv_cache is None:\n",
        "            # Full context (no cache)\n",
        "            if use_input_embeds is not None:\n",
        "                logits = self.model(inputs_embeds=use_input_embeds).logits\n",
        "            else:\n",
        "                logits = self.model(x.data).logits\n",
        "            return logits[:, block_loc.start:block_loc.end]\n",
        "            \n",
        "        elif kv_cache.cache_type == 'prefix':\n",
        "            # Prefix Cache: past_key_values contains context up to block_start\n",
        "            past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "            \n",
        "            if use_input_embeds is not None:\n",
        "                # Input embeddings should correspond to x[block_loc.start:]\n",
        "                logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            else:\n",
        "                logits = self.model(x[block_loc.start:], past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            \n",
        "            curr_len = block_loc.end - block_loc.start\n",
        "            return logits[:, :curr_len]\n",
        "\n",
        "        else:\n",
        "            # Dual/Sliding Cache: typically uses block context\n",
        "            past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "            \n",
        "            if use_input_embeds is not None:\n",
        "                 logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            else:\n",
        "                 # Use x slice instead of block to ensure we have the latest updates\n",
        "                 logits = self.model(x[block_loc.start:block_loc.end], past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            return logits\n",
        "\n",
        "    def validate_schedule(self, block_length, soft_token_ratio, treat_soft_tokens_as_candidates):\n",
        "        \"\"\" Validates that the decoding schedule can be satisfied with the given soft token ratio.\n",
        "        \"\"\"\n",
        "        # Only validate for FixedParallelDecoder which has steps\n",
        "        if not hasattr(self.decoder, 'steps') or treat_soft_tokens_as_candidates:\n",
        "            return\n",
        "\n",
        "        steps = self.decoder.steps\n",
        "        current_masks = block_length\n",
        "        \n",
        "        # Calculate the schedule for a full block\n",
        "        base = current_masks // steps\n",
        "        remainder = current_masks % steps\n",
        "        \n",
        "        schedule = []\n",
        "        for i in range(steps):\n",
        "            count = base + (1 if i < remainder else 0)\n",
        "            schedule.append(count)\n",
        "            \n",
        "        # Simulate decoding\n",
        "        for step_idx, num_to_decode in enumerate(schedule):\n",
        "            num_soft = int(current_masks * soft_token_ratio)\n",
        "            available = current_masks - num_soft\n",
        "            \n",
        "            if available < num_to_decode:\n",
        "                # Just warn instead of raising error to prevent crashing server\n",
        "                print(\n",
        "                    f\"Decoding Schedule Violation: Step {step_idx} requires decoding {num_to_decode} tokens, \"\n",
        "                    f\"but only {available} masks are available ({current_masks} total - {num_soft} soft tokens). \"\n",
        "                    f\"Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\"\n",
        "                )\n",
        "                return\n",
        "            current_masks -= num_to_decode\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, gen_length=128, block_length=128, soft_token_ratio=None, treat_soft_tokens_as_candidates=None, steps=None, threshold=None, soft_temperature=None):\n",
        "        ''' Generate tokens with diffusion iterations block by block using Soft Token Sampling.\n",
        "        '''\n",
        "        # Use instance defaults if not provided\n",
        "        if soft_token_ratio is None:\n",
        "            soft_token_ratio = self.soft_token_ratio\n",
        "        if treat_soft_tokens_as_candidates is None:\n",
        "            treat_soft_tokens_as_candidates = self.treat_soft_tokens_as_candidates\n",
        "        if soft_temperature is None:\n",
        "            soft_temperature = self.soft_temperature\n",
        "            \n",
        "        # Update decoder parameters\n",
        "        if steps is not None and hasattr(self.decoder, 'steps'):\n",
        "            self.decoder.steps = steps\n",
        "            \n",
        "        if threshold is not None and hasattr(self.decoder, 'threshold'):\n",
        "            self.decoder.threshold = threshold\n",
        "            \n",
        "        self.validate_schedule(block_length, soft_token_ratio, treat_soft_tokens_as_candidates)\n",
        "\n",
        "        x = TokenArray(prompt, gen_length, self.decoder.mask_id, self.decoder.eos_id, self.model.device)\n",
        "        it = self.iterator_factory.create(x, block_length)\n",
        "\n",
        "        iter_no = 0\n",
        "        kv_cache = self.cache_factory.create() if self.cache_factory is not None else None\n",
        "        \n",
        "        for block_id, (block_loc, block) in enumerate(it):\n",
        "            self.decoder.block_init(block, block_id)\n",
        "            \n",
        "            while (block == self.decoder.mask_id).sum() > 0:\n",
        "                \n",
        "                # Calculate unroll_k based on mask count and expected TPF\n",
        "                unroll_k = max(min((block == self.decoder.mask_id).sum()//self.expected_tpf, self.maximum_unroll), 1)\n",
        "                \n",
        "                for unroll_i in range(unroll_k):\n",
        "                    # Pre-check: Ensure we can satisfy the soft token ratio without violating the decoding schedule\n",
        "                    # if we choose to exclude soft tokens from candidacy.\n",
        "                    current_masks = (x[block_loc.start:block_loc.end] == self.decoder.mask_id).sum().item()\n",
        "                    \n",
        "                    # Optimization: If no masks left, stop unrolling (matches blockwise behavior)\n",
        "                    if current_masks == 0:\n",
        "                        break\n",
        "\n",
        "                    num_soft = int(current_masks * soft_token_ratio)\n",
        "                    \n",
        "                    # Determine num_to_decode for the current step\n",
        "                    num_to_decode = 0\n",
        "                    if hasattr(self.decoder, 'num_transfer_tokens'):\n",
        "                        # Fixed schedule\n",
        "                        if self.decoder.iter < self.decoder.num_transfer_tokens.shape[1]:\n",
        "                            num_to_decode = self.decoder.num_transfer_tokens[0, self.decoder.iter].item()\n",
        "                    else:\n",
        "                        # Dynamic schedule (Threshold decoder) - estimation not straightforward here without logits\n",
        "                        pass\n",
        "                    \n",
        "                    if not treat_soft_tokens_as_candidates and num_to_decode > 0:\n",
        "                        # If soft tokens CANNOT be decoded, we must have enough pure masks left to satisfy decoder demand\n",
        "                        available_for_decoding = current_masks - num_soft\n",
        "                        if available_for_decoding < num_to_decode:\n",
        "                            # Log warning instead of crashing\n",
        "                            print(\n",
        "                                f\"Decoding Schedule Violation: Step {self.decoder.iter} requires decoding {num_to_decode} tokens, \"\n",
        "                                f\"but only {available_for_decoding} masks are available ({current_masks} total - {num_soft} soft tokens). \"\n",
        "                                f\"Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\"\n",
        "                            )\n",
        "                            # Adjust num_soft to make it work\n",
        "                            num_soft = max(0, current_masks - num_to_decode)\n",
        "\n",
        "                    # 1. Handle KV Cache Update (Initial step for block or periodically)\n",
        "                    if kv_cache is not None and kv_cache.require_update(iter_no, block_loc.start, block_loc.end):\n",
        "                        output = self.model(x.data, use_cache=True)\n",
        "                        self.num_forwards += 1\n",
        "                        \n",
        "                        # Update cache\n",
        "                        kv_cache.update(output.past_key_values)\n",
        "                        self.cache_updates += 1\n",
        "                        \n",
        "                        # Decode using these initial logits (Standard dInfer behavior)\n",
        "                        self.decoder.decode(output.logits[:, block_loc.start:block_loc.end], block_loc.start, block_loc.end, x)\n",
        "\n",
        "                    # 2. Pass 1: Standard Logits (with current masks)\n",
        "                    logits1 = self._compute_logits(x, block_loc, kv_cache, use_input_embeds=None)\n",
        "                    self.num_forwards += 1\n",
        "                    \n",
        "                    decoding_logits = logits1\n",
        "                    soft_indices = None\n",
        "                    \n",
        "                    # 3. Soft Token Logic\n",
        "                    # Identify masks in the current block\n",
        "                    curr_block_ids = x[block_loc.start:block_loc.end]\n",
        "                    mask_mask = (curr_block_ids == self.decoder.mask_id)\n",
        "                    mask_indices = torch.nonzero(mask_mask).flatten() # Indices relative to block start\n",
        "                    \n",
        "                    if mask_indices.numel() > 0 and soft_token_ratio > 0:\n",
        "                        if num_soft > 0:\n",
        "                            perm = torch.randperm(mask_indices.numel(), device=self.model.device)\n",
        "                            soft_indices = mask_indices[perm[:num_soft]] # Indices relative to block start\n",
        "                            \n",
        "                            # Extract logits for these positions\n",
        "                            # logits1 shape: [1, block_len, vocab]\n",
        "                            selected_logits = logits1[0, soft_indices]\n",
        "                            \n",
        "                            # Apply soft temperature\n",
        "                            if soft_temperature > 0:\n",
        "                                selected_logits = selected_logits / soft_temperature\n",
        "\n",
        "                            probs = torch.softmax(selected_logits, dim=-1)\n",
        "                            \n",
        "                            # Compute Soft Embeddings: Weighted average of token embeddings\n",
        "                            # [num_soft, vocab] @ [vocab, d_model] -> [num_soft, d_model]\n",
        "                            soft_embeds = torch.matmul(probs, self.input_embeddings.weight)\n",
        "                            \n",
        "                            # Prepare Input Embeddings\n",
        "                            target_ids = None\n",
        "                            global_offset = 0\n",
        "                            \n",
        "                            if kv_cache is None:\n",
        "                                target_ids = x.data\n",
        "                                global_offset = block_loc.start # Offset in target_ids\n",
        "                            elif kv_cache.cache_type == 'prefix':\n",
        "                                target_ids = x[block_loc.start:]\n",
        "                                global_offset = 0 # relative to start of target_ids\n",
        "                            else:\n",
        "                                target_ids = curr_block_ids\n",
        "                                global_offset = 0\n",
        "                            \n",
        "                            # Get base embeddings for the input context\n",
        "                            inputs_embeds = self.input_embeddings(target_ids).clone() # [1, len, d_model]\n",
        "                            \n",
        "                            # Replace masks with soft embeddings\n",
        "                            inputs_embeds[0, global_offset + soft_indices] = soft_embeds\n",
        "                            \n",
        "                            # Pass 2: Get logits with Soft Tokens\n",
        "                            logits2 = self._compute_logits(x, block_loc, kv_cache, use_input_embeds=inputs_embeds)\n",
        "                            self.num_forwards += 1\n",
        "                            decoding_logits = logits2\n",
        "\n",
        "\n",
        "                    # Force EOS probability to zero (effectively) to prevent soft token averaging from including EOS\n",
        "                    # if hasattr(self.decoder, 'eos_id'):\n",
        "                    #     decoding_logits[:, :, self.decoder.eos_id] = -10000.0\n",
        "\n",
        "                    # 4. Decode using the latest logits\n",
        "                    if not treat_soft_tokens_as_candidates and soft_indices is not None and soft_indices.numel() > 0:\n",
        "                        # We want to prevent these indices from being selected.\n",
        "                        # Set logits for soft tokens to a uniform distribution (max entropy -> min confidence)\n",
        "                        decoding_logits_modified = decoding_logits.clone()\n",
        "                        decoding_logits_modified[0, soft_indices] = 0.1 \n",
        "                        \n",
        "                        self.decoder.decode(decoding_logits_modified, block_loc.start, block_loc.end, x)\n",
        "                    else:\n",
        "                        self.decoder.decode(decoding_logits, block_loc.start, block_loc.end, x)\n",
        "                        \n",
        "                    iter_no += 1\n",
        "\n",
        "            # Early stop at EOS\n",
        "            if self.early_stop and torch.any(x[block_loc.start:block_loc.end] == self.decoder.eos_id):\n",
        "                x[block_loc.end:] = self.decoder.eos_id\n",
        "                break\n",
        "\n",
        "        # DEBUG: Check for EOS tokens to explain short output\n",
        "        eos_count = (x.data == self.decoder.eos_id).sum().item()\n",
        "        if eos_count > 0:\n",
        "            total_len = x.total_length\n",
        "            print(f\"SoftTokenLLM Generated {eos_count} EOS tokens out of {total_len} total positions. \"\n",
        "                  f\"This will shorten the output by {eos_count} tokens.\")\n",
        "                           \n",
        "        return x.get_generated_tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating with Soft Token LLM...\n",
            "Prompt length: 33 tokens\n",
            "Generating up to 1024 tokens...\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "Decoding Schedule Violation: Step 31 requires decoding 2 tokens, but only 1 masks are available (2 total - 1 soft tokens). Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\n",
            "SoftTokenLLM Generated 988 EOS tokens out of 1057 total positions. This will shorten the output by 988 tokens.\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 36\n",
            "- Forward passes: 992\n",
            "- Cache updates: 16\n",
            "- Time: 38.93s\n",
            "- Tokens/second: 0.92\n",
            "- Forwards/second: 25.48\n",
            "\n",
            "Generated: <|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the derivative of x^2 + 2x + 1 with respect to x?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "To find the derivative of the function \\( x^2 + 2x + 1 \\) with respect to \\( x \\), we will apply the basic rules of differentiation.<|eot_id|>\n",
            "--------------------------------\n",
            "Generating with Block Wise LLM...\n",
            "Prompt length: 33 tokens\n",
            "Generating up to 1024 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 324\n",
            "- Forward passes: 512\n",
            "- Cache updates: 16\n",
            "- Time: 20.01s\n",
            "- Tokens/second: 16.19\n",
            "- Forwards/second: 25.59\n",
            "\n",
            "Generated: <|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the derivative of x^2 + 2x + 1 with respect to x?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "To find the derivative of the function \\( x^2 + 2x + 1 \\) with respect to \\( x \\), we will apply the power rule and the sum rule of differentiation.\n",
            "\n",
            "The power rule states that if \\( f(x) = x^n \\), then \\( f'(x) = nx^{n-1} \\).\n",
            "\n",
            "The sum rule states that if \\( f(x) = g(x) + h(x) \\), then \\( f'(x) = g'(x) + h'(x) \\).\n",
            "\n",
            "Let's differentiate each term of the function \\( x^2 + 2x + 1 \\):\n",
            "\n",
            "1. For \\( x^2 \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(x^2) = 2x\n",
            "   \\]\n",
            "\n",
            "2. For \\( 2x \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(2x) = 2\n",
            "   \\]\n",
            "\n",
            "3. For the constant term \\( 1 \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(1) = 0\n",
            "   \\]\n",
            "\n",
            "Now, we combine these derivatives:\n",
            "\\[\n",
            "\\frac{d}{dx}(x^2 + 2x + 1) = 2x + 2 + 0 = 2x + 2\n",
            "\\]\n",
            "\n",
            "Therefore, the derivative of \\( x^2 + 2x + 1 \\) with respect to \\( x \\) is:\n",
            "\\[\n",
            "f'(x) = 2x + 2\n",
            "\\]<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "generation_config = {\n",
        "    \"gen_length\": 1024,      # Maximum number of tokens to generate\n",
        "    \"steps\": 64,\n",
        "    \"block_length\": 64,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": False,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "    \"treat_soft_tokens_as_candidates\": False,\n",
        "    \"soft_temperature\": 0.2,\n",
        "    \"soft_token_ratio\": 0.5,\n",
        "}\n",
        "\n",
        "test_prompt = \"What is the derivative of x^2 + 2x + 1 with respect to x?\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "soft_llm = BlockWiseSoftTokenLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    soft_token_ratio=generation_config[\"soft_token_ratio\"],\n",
        "    treat_soft_tokens_as_candidates=generation_config[\"treat_soft_tokens_as_candidates\"],\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        "    soft_temperature=generation_config[\"soft_temperature\"],\n",
        ")\n",
        "\n",
        "block_wise_llm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        ")\n",
        "\n",
        "# Generate\n",
        "print(\"Generating with Soft Token LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=soft_llm, config=generation_config)\n",
        "print(f\"\\nGenerated: {generated}\")\n",
        "\n",
        "print(\"--------------------------------\")\n",
        "\n",
        "print(\"Generating with Block Wise LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=block_wise_llm, config=generation_config)\n",
        "print(f\"\\nGenerated: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLURM Job Kernel (root)",
      "language": "python",
      "name": "slurm-job-kernel-mfathi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
