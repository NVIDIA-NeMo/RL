{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dInfer Soft Token Experimentation Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Added dInfer to Python path: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python\n",
            "✓ Cleared cached dinfer modules\n",
            "✓ dInfer module found at: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python/dinfer/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Add dInfer to Python path from the 3rdparty submodule\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the dInfer python directory to sys.path\n",
        "DINFER_PATH = os.path.abspath('3rdparty/dInfer/python')\n",
        "if os.path.exists(DINFER_PATH):\n",
        "    if DINFER_PATH not in sys.path:\n",
        "        sys.path.insert(0, DINFER_PATH)\n",
        "    print(f\"✓ Added dInfer to Python path: {DINFER_PATH}\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: dInfer path not found: {DINFER_PATH}\")\n",
        "    print(\"  Make sure you're running this notebook from the project root.\")\n",
        "\n",
        "# Clear any cached imports to ensure we use the latest code\n",
        "import importlib\n",
        "for module_name in list(sys.modules.keys()):\n",
        "    if 'dinfer' in module_name:\n",
        "        del sys.modules[module_name]\n",
        "print(\"✓ Cleared cached dinfer modules\")\n",
        "\n",
        "# Verify the import works\n",
        "try:\n",
        "    import dinfer\n",
        "    print(f\"✓ dInfer module found at: {dinfer.__file__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import dInfer: {e}\")\n",
        "\n",
        "# Note: You can also use the import utilities from xp/llada_api/llada_generate/dinfer/_imports.py\n",
        "# which provides the same functionality with additional error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch compilation disabled to avoid backend issues\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Disable torch compilation to avoid backend compiler errors\n",
        "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
        "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
        "\n",
        "# Disable torch.compile globally\n",
        "torch._dynamo.config.disable = True\n",
        "\n",
        "# dinfer imports (using local dInfer from 3rdparty/dInfer/python added to path above)\n",
        "from dinfer.model import LLaDAModelLM\n",
        "from dinfer.decoding.parallel_strategy import (\n",
        "    ParallelDecoder,\n",
        "    ThresholdParallelDecoder,\n",
        "    CreditThresholdParallelDecoder,\n",
        "    HierarchyDecoder,\n",
        "    get_num_transfer_tokens,\n",
        "    get_transfer_index,\n",
        ")\n",
        "from dinfer import (\n",
        "    BlockWiseDiffusionLLM,\n",
        "    BlockIteratorFactory,\n",
        "    KVCacheFactory,\n",
        "    SlidingWindowDiffusionLLM,\n",
        ")\n",
        "\n",
        "from dinfer.decoding.utils import (\n",
        "    TokenArray,\n",
        ")\n",
        "\n",
        "print(\"Torch compilation disabled to avoid backend issues\")\n",
        "\n",
        "# LLaDA tokenizer constants\n",
        "MASK_ID = 126336\n",
        "EOS_ID = 126081\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from GSAI-ML/LLaDA-8B-Instruct...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 198.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "MODEL_PATH = \"GSAI-ML/LLaDA-8B-Instruct\"  # Update this path to your model\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "model = LLaDAModelLM.from_pretrained(\n",
        "    MODEL_PATH, \n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16, \n",
        "    init_device=str(device)  # Convert device to string for JSON serialization\n",
        ").eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: Compile the model for better performance\n",
        "# model = torch.compile(model, mode='reduce-overhead', fullgraph=True)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder and generation pipeline configured!\n"
          ]
        }
      ],
      "source": [
        "# Generation parameters - EXPERIMENT WITH THESE!\n",
        "generation_config = {\n",
        "    \"gen_length\": 256,      # Maximum number of tokens to generate\n",
        "    \"steps\": 64,\n",
        "    \"block_length\": 64,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": True,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "}\n",
        "\n",
        "# Create decoder with threshold strategy\n",
        "decoder = ThresholdParallelDecoder(0, threshold=generation_config[\"threshold\"], mask_id=MASK_ID, eos_id=EOS_ID)\n",
        "\n",
        "# Alternative: Use FixedParallelDecoderWithEOS for fixed-step decoding\n",
        "# decoder = FixedParallelDecoderWithEOS(\n",
        "#     temperature=0,\n",
        "#     steps=generation_config[\"steps\"],\n",
        "#     mask_id=MASK_ID,\n",
        "#     eos_id=EOS_ID\n",
        "# )\n",
        "# print(\"Using FixedParallelDecoderWithEOS\")\n",
        "\n",
        "# Create iterator factory\n",
        "iterator_factory = BlockIteratorFactory(True)\n",
        "\n",
        "# Create KV cache factory if using caching\n",
        "cache_factory = KVCacheFactory(generation_config[\"cache_type\"]) if generation_config[\"cache_type\"] else None\n",
        "\n",
        "# Create the Diffusion LLM instance\n",
        "dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        "    maximum_unroll=generation_config[\"maximum_unroll\"],\n",
        "    expected_tpf=generation_config[\"expected_tpf\"]\n",
        ")\n",
        "\n",
        "print(\"Decoder and generation pipeline configured!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt length: 21 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 255\n",
            "- Forward passes: 234\n",
            "- Cache updates: 4\n",
            "- Time: 6.90s\n",
            "- Tokens/second: 36.98\n",
            "- Forwards/second: 33.93\n",
            "\n",
            "Generated text:\n",
            "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Once upon a time in a magical forest<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Once upon a time in a magical forest, where the trees whispered secrets and the streams sang lullabies, there lived a young girl named Lily. She was known throughout the forest for her kindness and her adventurous spirit. One day, while exploring the forest, she stumbled upon a hidden path that led to a sparkling pond. The water shimmered like diamonds in the sunlight, and the air was filled with the scent of blooming flowers.\n",
            "\n",
            "As Lily approached the pond, she noticed a small, glowing fish swimming gracefully in the water. Curious, she reached out and gently touched the fish. To her surprise, the fish began to glow brighter, and a soft, melodic voice echoed through the forest.\n",
            "\n",
            "\"Welcome, dear Lily,\" the voice said. \"You have touched the heart of the forest, and now you are worthy to learn its secrets.\"\n",
            "\n",
            "Lily felt a surge of excitement and curiosity. She knew that she had been chosen by the forest to uncover its mysteries. With a deep breath, she stepped into the forest, ready to embark on a journey of discovery and wonder.\n",
            "\n",
            "And so, Lily's adventure in the magical forest began, filled with magic, mystery, and the promise of endless wonders waiting to be discovered.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(prompt, dllm_instance=dllm, tokenizer=tokenizer, config=generation_config):\n",
        "    \"\"\"\n",
        "    Generate text using the diffusion LLM.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        dllm_instance: Diffusion LLM instance\n",
        "        tokenizer: Tokenizer instance\n",
        "        config: Generation configuration dict\n",
        "    \n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    message = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False)\n",
        "    input_ids = tokenizer(message, return_tensors=\"pt\")['input_ids'].to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "    \n",
        "    print(f\"Prompt length: {prompt_length} tokens\")\n",
        "    print(f\"Generating up to {config['gen_length']} tokens...\")\n",
        "    \n",
        "    # Track statistics\n",
        "    prev_forwards = dllm_instance.num_forwards\n",
        "    prev_cache_updates = dllm_instance.cache_updates\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    output_ids = dllm_instance.generate(\n",
        "        input_ids, \n",
        "        gen_length=config['gen_length'], \n",
        "        block_length=config['block_length']\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_forwards = dllm_instance.num_forwards - prev_forwards\n",
        "    total_cache_updates = dllm_instance.cache_updates - prev_cache_updates\n",
        "    generated_tokens = output_ids.shape[1] - prompt_length\n",
        "    generation_time = end_time - start_time\n",
        "    \n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nGeneration Statistics:\")\n",
        "    print(f\"- Generated tokens: {generated_tokens}\")\n",
        "    print(f\"- Forward passes: {total_forwards}\")\n",
        "    print(f\"- Cache updates: {total_cache_updates}\")\n",
        "    print(f\"- Time: {generation_time:.2f}s\")\n",
        "    print(f\"- Tokens/second: {generated_tokens/generation_time:.2f}\")\n",
        "    print(f\"- Forwards/second: {total_forwards/generation_time:.2f}\")\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "# Test the generation function\n",
        "test_prompt = \"Once upon a time in a magical forest\"\n",
        "generated = generate_text(test_prompt)\n",
        "print(f\"\\nGenerated text:\\n{generated}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT: The future of artificial intelligence is\n",
            "================================================================================\n",
            "Prompt length: 6 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 9\n",
            "- Forward passes: 36\n",
            "- Cache updates: 1\n",
            "- Time: 1.04s\n",
            "- Tokens/second: 8.64\n",
            "- Forwards/second: 34.57\n",
            "\n",
            "GENERATED:\n",
            "The future of artificial intelligence is bright, and the possibilities are endless.<|eot_id|>\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Interactive prompt examples\n",
        "prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Write a haiku about programming:\",\n",
        "    \"The most important scientific discovery was\",\n",
        "]\n",
        "\n",
        "# Generate for each prompt\n",
        "for prompt in prompts[:1]:  # Change to prompts to run all\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    generated = generate_text(prompt)\n",
        "    print(f\"\\nGENERATED:\\n{generated}\")\n",
        "    print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.7\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 100\n",
            "- Cache updates: 3\n",
            "- Time: 2.89s\n",
            "- Tokens/second: 45.05\n",
            "- Forwards/second: 34.65\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.8\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 113\n",
            "- Cache updates: 3\n",
            "- Time: 3.26s\n",
            "- Tokens/second: 39.91\n",
            "- Forwards/second: 34.69\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.9\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 122\n",
            "- Cache updates: 3\n",
            "- Time: 3.51s\n",
            "- Tokens/second: 36.99\n",
            "- Forwards/second: 34.72\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.95\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 130\n",
            "- Forward passes: 130\n",
            "- Cache updates: 3\n",
            "- Time: 3.75s\n",
            "- Tokens/second: 34.69\n",
            "- Forwards/second: 34.69\n",
            "\n",
            "Generated:  a deeply personal and subjective question, and it can vary from person to person. For some, the meaning of life may be to find happiness, love, and fulfillment. For others, it may be to make a positive impact on the world or to achieve a sense of purpose and meaning.\n",
            "\n",
            "It's important to remember that the meaning of life is not a fixed or absolute concept. Rather, it's something that can be explored and evolved over time, as we grow, learn, and have new experiences.\n",
            "\n",
            "Ultimately, the meaning of life is something that each person must discover for themselves, through exploration, reflection, and experience.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different threshold values\n",
        "thresholds = [0.7, 0.8, 0.9, 0.95]\n",
        "test_prompt = \"The meaning of life is\"\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing with threshold = {threshold}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create new decoder with different threshold\n",
        "    test_decoder = ThresholdParallelDecoder(0, threshold=threshold)\n",
        "    \n",
        "    # Create new DLLM instance\n",
        "    test_dllm = BlockWiseDiffusionLLM(\n",
        "        model=model,\n",
        "        decoder=test_decoder,\n",
        "        iterator_factory=iterator_factory,\n",
        "        cache_factory=cache_factory,\n",
        "        early_stop=True\n",
        "    )\n",
        "    \n",
        "    # Generate and compare\n",
        "    generated = generate_text(test_prompt, dllm_instance=test_dllm)\n",
        "    print(f\"\\nGenerated: {generated[len(test_prompt):]}\")  # Show only generated part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block-wise generation:\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 70\n",
            "- Forward passes: 83\n",
            "- Cache updates: 2\n",
            "- Time: 2.40s\n",
            "- Tokens/second: 29.12\n",
            "- Forwards/second: 34.53\n",
            "\n",
            "\n",
            "Sliding window generation:\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 67\n",
            "- Forward passes: 74\n",
            "- Cache updates: 2\n",
            "- Time: 2.16s\n",
            "- Tokens/second: 31.07\n",
            "- Forwards/second: 34.32\n",
            "\n",
            "\n",
            "Comparison:\n",
            "Block-wise output:  the way we live, work, and interact with each other. It will enable us to solve complex problems, i...\n",
            "Sliding window output:  the way we live, work, and interact with each other. It will enable us to be more efficient, produc...\n"
          ]
        }
      ],
      "source": [
        "# Create sliding window DLLM\n",
        "sliding_dllm = SlidingWindowDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=KVCacheFactory('dual'),  # Sliding window requires cache\n",
        "    prefix_look=0,      # How many tokens to look back\n",
        "    after_look=0,       # How many tokens to look ahead\n",
        "    warmup_steps=1,     # Warmup iterations\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Compare block-wise vs sliding window\n",
        "test_prompt = \"Artificial intelligence will revolutionize\"\n",
        "\n",
        "print(\"Block-wise generation:\")\n",
        "print(\"=\"*80)\n",
        "block_generated = generate_text(test_prompt, dllm_instance=dllm)\n",
        "\n",
        "print(\"\\n\\nSliding window generation:\")\n",
        "print(\"=\"*80)\n",
        "sliding_generated = generate_text(test_prompt, dllm_instance=sliding_dllm)\n",
        "\n",
        "print(\"\\n\\nComparison:\")\n",
        "print(f\"Block-wise output: {block_generated[len(test_prompt):][:100]}...\")\n",
        "print(f\"Sliding window output: {sliding_generated[len(test_prompt):][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _FixedParallelDecoder(ParallelDecoder):\n",
        "    \"\"\" This decoder decodes tokens in a fixed number of steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature, steps, remasking='low_confidence', mask_id=MASK_ID, eos_id=EOS_ID):\n",
        "        super().__init__(temperature, remasking, mask_id)\n",
        "        self.steps = steps\n",
        "        self.iter = 0\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def block_init(self, block_x, block_id):\n",
        "        # TODO(zhengda) we need to handle steps correctly here when the distributed version changes the gen length.\n",
        "        block_mask_index = block_x == self.mask_id\n",
        "        self.num_transfer_tokens = get_num_transfer_tokens(block_mask_index, self.steps)\n",
        "        self.iter = 0\n",
        "\n",
        "    def decode(self, logits, block_start, block_end, x, iter_threshold = None):\n",
        "        \"\"\" Decode the logits in a block.\n",
        "        \"\"\"\n",
        "        mask_index = (x[block_start:block_end] == self.mask_id)\n",
        "        assert mask_index.shape[1] == logits.shape[1]\n",
        "\n",
        "        curr_x = x[block_start:block_end]\n",
        "        x0, transfer_index = get_transfer_index(logits, self.temperature, self.remasking, mask_index, curr_x, self.num_transfer_tokens[:, self.iter], None)\n",
        "        self.iter += 1\n",
        "        x[block_start:block_end][transfer_index] = x0[transfer_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FixedParallelDecoder with different fixed ratios...\n",
            "Prompt length: 20 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 68\n",
            "- Forward passes: 128\n",
            "- Cache updates: 2\n",
            "- Time: 3.83s\n",
            "- Tokens/second: 17.77\n",
            "- Forwards/second: 33.45\n",
            "\n",
            "Generated: id|>user<|end_header_id|>\n",
            "\n",
            "What is the key to innovation?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The key to innovation is a combination of creativity, curiosity, and a willingness to take risks. It involves the ability to think outside the box, challenge existing assumptions, and explore new ideas and approaches. Additionally, it requires a commitment to continuous learning, experimentation, and collaboration, as well as a willingness to learn from failures and setbacks.<|eot_id|>...\n"
          ]
        }
      ],
      "source": [
        "# Experiment with FixedParallelDecoder if available\n",
        "print(\"Testing FixedParallelDecoder with different fixed ratios...\")\n",
        "\n",
        "test_prompt = \"What is the key to innovation?\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "# Create DLLM with fixed decoder\n",
        "fixed_dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Generate\n",
        "generated = generate_text(test_prompt, dllm_instance=fixed_dllm)\n",
        "print(f\"\\nGenerated: {generated[len(test_prompt):]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft Token Diffusion Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlockWiseSoftTokenLLM:\n",
        "    def __init__(self, model, decoder, iterator_factory, early_stop=True, cache_factory=None, maximum_unroll=4, expected_tpf=8, soft_token_ratio=0.2, treat_soft_tokens_as_candidates=False):\n",
        "        self.model = model\n",
        "        self.cache_factory = cache_factory\n",
        "        self.decoder = decoder\n",
        "        self.iterator_factory = iterator_factory\n",
        "        self.num_forwards = 0\n",
        "        self.cache_updates = 0\n",
        "        self.early_stop = early_stop\n",
        "        self.maximum_unroll = maximum_unroll\n",
        "        self.expected_tpf = expected_tpf\n",
        "        self.soft_token_ratio = soft_token_ratio\n",
        "        self.treat_soft_tokens_as_candidates = treat_soft_tokens_as_candidates\n",
        "        self.input_embeddings = self.model.get_input_embeddings()\n",
        "\n",
        "    def validate_schedule(self, block_length):\n",
        "        \"\"\" Validates that the decoding schedule can be satisfied with the given soft token ratio.\n",
        "        \"\"\"\n",
        "        if not hasattr(self.decoder, 'steps') or self.treat_soft_tokens_as_candidates:\n",
        "            return\n",
        "\n",
        "        steps = self.decoder.steps\n",
        "        current_masks = block_length\n",
        "        \n",
        "        # Calculate the schedule for a full block\n",
        "        base = current_masks // steps\n",
        "        remainder = current_masks % steps\n",
        "        \n",
        "        schedule = []\n",
        "        for i in range(steps):\n",
        "            count = base + (1 if i < remainder else 0)\n",
        "            schedule.append(count)\n",
        "            \n",
        "        # Simulate decoding\n",
        "        for step_idx, num_to_decode in enumerate(schedule):\n",
        "            num_soft = int(current_masks * self.soft_token_ratio)\n",
        "            available = current_masks - num_soft\n",
        "            \n",
        "            if available < num_to_decode:\n",
        "                raise ValueError(\n",
        "                    f\"Decoding Schedule Violation: Step {step_idx} requires decoding {num_to_decode} tokens, \"\n",
        "                    f\"but only {available} masks are available ({current_masks} total - {num_soft} soft tokens). \"\n",
        "                    f\"Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\"\n",
        "                )\n",
        "            current_masks -= num_to_decode\n",
        "        \n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, gen_length=128, block_length=128):\n",
        "        ''' Generate tokens with diffusion iterations block by block using Soft Token Sampling.\n",
        "        '''\n",
        "        self.validate_schedule(block_length)\n",
        "\n",
        "        x = TokenArray(prompt, gen_length, self.decoder.mask_id, self.decoder.eos_id, self.model.device)\n",
        "        it = self.iterator_factory.create(x, block_length)\n",
        "\n",
        "        iter_no = 0\n",
        "        kv_cache = self.cache_factory.create() if self.cache_factory is not None else None\n",
        "        \n",
        "        for block_id, (block_loc, block) in enumerate(it):\n",
        "            self.decoder.block_init(block, block_id)\n",
        "\n",
        "            while (block == self.decoder.mask_id).sum() > 0:\n",
        "                \n",
        "                # Pre-check: Ensure we can satisfy the soft token ratio without violating the decoding schedule\n",
        "                # if we choose to exclude soft tokens from candidacy.\n",
        "                current_masks = (x[block_loc.start:block_loc.end] == self.decoder.mask_id).sum().item()\n",
        "                num_soft = int(current_masks * self.soft_token_ratio)\n",
        "                \n",
        "                # The decoder wants to transfer (decode) N tokens this step\n",
        "                num_to_decode = self.decoder.num_transfer_tokens[0, self.decoder.iter].item()\n",
        "                \n",
        "                if not self.treat_soft_tokens_as_candidates:\n",
        "                    # If soft tokens CANNOT be decoded, we must have enough pure masks left to satisfy decoder demand\n",
        "                    available_for_decoding = current_masks - num_soft\n",
        "                    if available_for_decoding < num_to_decode:\n",
        "                        raise ValueError(\n",
        "                            f\"Decoding Schedule Violation: Step {self.decoder.iter} requires decoding {num_to_decode} tokens, \"\n",
        "                            f\"but only {available_for_decoding} masks are available ({current_masks} total - {num_soft} soft tokens). \"\n",
        "                            f\"Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\"\n",
        "                        )\n",
        "                \n",
        "                # Helper to run model with correct context and embeddings\n",
        "                def run_model(use_input_embeds=None):\n",
        "                    # Determine input context based on cache type\n",
        "                    if kv_cache is None:\n",
        "                        # Full context (no cache)\n",
        "                        if use_input_embeds is not None:\n",
        "                            logits = self.model(inputs_embeds=use_input_embeds).logits\n",
        "                        else:\n",
        "                            logits = self.model(x.data).logits\n",
        "                        return logits[:, block_loc.start:block_loc.end]\n",
        "                        \n",
        "                    elif kv_cache.cache_type == 'prefix':\n",
        "                        # Prefix Cache: past_key_values contains context up to block_start\n",
        "                        past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "                        \n",
        "                        if use_input_embeds is not None:\n",
        "                            # Input embeddings should correspond to x[block_loc.start:]\n",
        "                            logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        else:\n",
        "                            logits = self.model(x[block_loc.start:], past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        \n",
        "                        curr_len = block_loc.end - block_loc.start\n",
        "                        return logits[:, :curr_len]\n",
        "\n",
        "                    else:\n",
        "                        # Dual/Sliding Cache: typically uses block context\n",
        "                        past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "                        \n",
        "                        if use_input_embeds is not None:\n",
        "                             logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        else:\n",
        "                             logits = self.model(block, past_key_values=past_key_values, use_cache=True,\n",
        "                                              replace_position=replace_position).logits\n",
        "                        return logits\n",
        "\n",
        "                # 1. Handle KV Cache Update (Initial step for block or periodically)\n",
        "                if kv_cache is not None and kv_cache.require_update(iter_no, block_loc.start, block_loc.end):\n",
        "                    output = self.model(x.data, use_cache=True)\n",
        "                    self.num_forwards += 1\n",
        "                    \n",
        "                    # Update cache\n",
        "                    kv_cache.update(output.past_key_values)\n",
        "                    self.cache_updates += 1\n",
        "                    \n",
        "                    # Decode using these initial logits\n",
        "                    self.decoder.decode(output.logits[:, block_loc.start:block_loc.end], block_loc.start, block_loc.end, x)\n",
        "                    iter_no += 1\n",
        "                    continue\n",
        "\n",
        "                # 2. Pass 1: Standard Logits (with current masks)\n",
        "                logits1 = run_model(use_input_embeds=None)\n",
        "                self.num_forwards += 1\n",
        "                \n",
        "                decoding_logits = logits1\n",
        "                soft_indices = None\n",
        "                \n",
        "                # 3. Soft Token Logic\n",
        "                # Identify masks in the current block\n",
        "                curr_block_ids = x[block_loc.start:block_loc.end]\n",
        "                mask_mask = (curr_block_ids == self.decoder.mask_id)\n",
        "                mask_indices = torch.nonzero(mask_mask).flatten() # Indices relative to block start\n",
        "                \n",
        "                if mask_indices.numel() > 0 and self.soft_token_ratio > 0:\n",
        "                    if num_soft > 0:\n",
        "                        perm = torch.randperm(mask_indices.numel(), device=self.model.device)\n",
        "                        soft_indices = mask_indices[perm[:num_soft]] # Indices relative to block start\n",
        "                        \n",
        "                        # Extract logits for these positions\n",
        "                        # logits1 shape: [1, block_len, vocab]\n",
        "                        selected_logits = logits1[0, soft_indices]\n",
        "                        probs = torch.softmax(selected_logits, dim=-1)\n",
        "                        \n",
        "                        # Compute Soft Embeddings: Weighted average of token embeddings\n",
        "                        # [num_soft, vocab] @ [vocab, d_model] -> [num_soft, d_model]\n",
        "                        soft_embeds = torch.matmul(probs, self.input_embeddings.weight)\n",
        "                        \n",
        "                        # Prepare Input Embeddings\n",
        "                        target_ids = None\n",
        "                        global_offset = 0\n",
        "                        \n",
        "                        if kv_cache is None:\n",
        "                            target_ids = x.data\n",
        "                            global_offset = block_loc.start # Offset in target_ids\n",
        "                        elif kv_cache.cache_type == 'prefix':\n",
        "                            target_ids = x[block_loc.start:]\n",
        "                            global_offset = 0 # relative to start of target_ids\n",
        "                        else:\n",
        "                            target_ids = curr_block_ids\n",
        "                            global_offset = 0\n",
        "                        \n",
        "                        # Get base embeddings for the input context\n",
        "                        inputs_embeds = self.input_embeddings(target_ids).clone() # [1, len, d_model]\n",
        "                        \n",
        "                        # Replace masks with soft embeddings\n",
        "                        inputs_embeds[0, global_offset + soft_indices] = soft_embeds\n",
        "                        \n",
        "                        # Pass 2: Get logits with Soft Tokens\n",
        "                        logits2 = run_model(use_input_embeds=inputs_embeds)\n",
        "                        self.num_forwards += 1\n",
        "                        decoding_logits = logits2\n",
        "\n",
        "                # 4. Decode using the latest logits\n",
        "                # IMPORTANT: If treat_soft_tokens_as_candidates is False, we must mask out the logits\n",
        "                # for soft tokens so they are not selected by the decoder (which picks top-k confidence usually)\n",
        "                # However, FixedParallelDecoder usually relies on 'transfer_index' which is determined by confidence.\n",
        "                # We can force the logits of soft tokens to be very low confidence (high entropy) or -inf\n",
        "                # but FixedParallelDecoder logic for selection is 'get_transfer_index'.\n",
        "                \n",
        "                if not self.treat_soft_tokens_as_candidates and soft_indices is not None and soft_indices.numel() > 0:\n",
        "                    # We want to prevent these indices from being selected.\n",
        "                    # The decoder selects based on confidence. We can artificially lower the confidence \n",
        "                    # of soft tokens to -inf (or uniform distribution) so they are last in line.\n",
        "                    # But we can't modify logits in-place if they are used for next step, so clone or careful modify.\n",
        "                    \n",
        "                    # Actually, simpler: mask them out in the 'mask_index' passed to get_transfer_index inside decode?\n",
        "                    # But we can't change the decoder code easily from here without subclassing.\n",
        "                    # Best approach: Modify the logits passed to decode() so that soft token positions look like garbage.\n",
        "                    \n",
        "                    # Set logits for soft tokens to a uniform distribution (max entropy -> min confidence)\n",
        "                    # or set them to -inf if we want to be sure.\n",
        "                    # But FixedParallelDecoder calculates confidence. \n",
        "                    \n",
        "                    # Let's just zero out their logits to make them uniform -> low confidence\n",
        "                    # decoding_logits is [1, len, vocab]\n",
        "                    # We have soft_indices relative to block start.\n",
        "                    \n",
        "                    # Create a mask for soft tokens\n",
        "                    # Set their logits to 0 (uniform probability after softmax, low max-prob)\n",
        "                    decoding_logits_modified = decoding_logits.clone()\n",
        "                    decoding_logits_modified[0, soft_indices] = -1000.0 # Effectively zero probability for all tokens\n",
        "                    \n",
        "                    self.decoder.decode(decoding_logits_modified, block_loc.start, block_loc.end, x)\n",
        "                else:\n",
        "                    self.decoder.decode(decoding_logits, block_loc.start, block_loc.end, x)\n",
        "                    \n",
        "                iter_no += 1\n",
        "\n",
        "            # Early stop at EOS\n",
        "            if self.early_stop and torch.any(x[block_loc.start:block_loc.end] == self.decoder.eos_id):\n",
        "                x[block_loc.end:] = self.decoder.eos_id\n",
        "                break\n",
        "\n",
        "        logger.info(f'SoftTokenLLM - Total diffusion iterations: {self.num_forwards}')\n",
        "        return x.get_generated_tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating with Soft Token LLM...\n",
            "Prompt length: 33 tokens\n",
            "Generating up to 1024 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 301\n",
            "- Forward passes: 192\n",
            "- Cache updates: 3\n",
            "- Time: 13.09s\n",
            "- Tokens/second: 22.99\n",
            "- Forwards/second: 14.67\n",
            "\n",
            "Generated: <|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the derivative of x^2 + 2x + 1 with respect to x?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "To find the derivative of the function \\( x^2 + 2x + 1 \\) with respect to \\( x \\), we will apply the power rule and the sum rule of differentiation.\n",
            "\n",
            "The power rule states that if \\( f(x) = x^n \\), then \\( f'(x) = nx^{n-1} \\).\n",
            "\n",
            "The sum rule states that if \\( f(x) = g(x) + h(x) \\), then \\( f'(x) = g'(x) + h'(x) \\).\n",
            "\n",
            "Let's differentiate each term of the function:\n",
            "\n",
            "1. Differentiate \\( x^2 \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(x^2) = 2x\n",
            "   \\]\n",
            "\n",
            "2. Differentiate \\( 2x \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(2x) = 2\n",
            "   \\]\n",
            "\n",
            "3. Differentiate the constant term \\( 1 \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(1) = 0\n",
            "   \\]\n",
            "\n",
            "Now, combine these results to get the derivative of the entire function:\n",
            "\\[\n",
            "\\frac{d}{dx}(x^2 + 2x + 1) = 2x + 2\n",
            "\\]\n",
            "\n",
            "So, the derivative of \\( x^2 + 2x + 1 \\) with respect to \\( x \\) is \\( 2x + 2 \\).<|eot_id|>\n",
            "--------------------------------\n",
            "Generating with Block Wise LLM...\n",
            "Prompt length: 33 tokens\n",
            "Generating up to 1024 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 257\n",
            "- Forward passes: 96\n",
            "- Cache updates: 3\n",
            "- Time: 6.60s\n",
            "- Tokens/second: 38.96\n",
            "- Forwards/second: 14.55\n",
            "\n",
            "Generated: <|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the derivative of x^2 + 2x + 1 with respect to x?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "To find the derivative of the function \\( x^2 + 2x + 1 \\) with respect to \\( x \\), we will use the power rule for differentiation. The power rule states that if \\( f(x) = x^n \\), then \\( f'(x) = nx^{n-1} \\).\n",
            "\n",
            "Let's differentiate each term of the function:\n",
            "\n",
            "1. For \\( x = x^2 \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(x^2) = 2x\n",
            "   \\]\n",
            "\n",
            "2. For \\( 2x \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(2x) = 2 \\cdot \\]\n",
            "\n",
            "3. For the constant \\( 1 \\):\n",
            "   \\[\n",
            "   \\frac{d}{dx}(1) = 0\n",
            "   \\]\n",
            "\n",
            "Now, we combine these results to get the derivative of the entire function:\n",
            "\\[\n",
            "\\frac{d}{dx}(x^2 + 2x + 1) = 2x + 2\n",
            "\\]\n",
            "\n",
            "So, the derivative of \\( x^2 + 2x + 1 \\) with respect to \\( x \\) is \\( 2x + 2 \\).<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "generation_config = {\n",
        "    \"gen_length\": 1024,      # Maximum number of tokens to generate\n",
        "    \"steps\": 32,\n",
        "    \"block_length\": 128,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": True,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "    \"treat_soft_tokens_as_candidates\": False,\n",
        "}\n",
        "\n",
        "test_prompt = \"What is the derivative of x^2 + 2x + 1 with respect to x?\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "soft_llm = BlockWiseSoftTokenLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True,\n",
        "    soft_token_ratio=0.9,\n",
        "    treat_soft_tokens_as_candidates=generation_config[\"treat_soft_tokens_as_candidates\"],\n",
        ")\n",
        "\n",
        "block_wise_llm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True,\n",
        ")\n",
        "\n",
        "# Generate\n",
        "print(\"Generating with Soft Token LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=fixed_dllm, config=generation_config)\n",
        "print(f\"\\nGenerated: {generated}\")\n",
        "\n",
        "print(\"--------------------------------\")\n",
        "\n",
        "print(\"Generating with Block Wise LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=block_wise_llm, config=generation_config)\n",
        "print(f\"\\nGenerated: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLURM Job Kernel (root)",
      "language": "python",
      "name": "slurm-job-kernel-mfathi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
