{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# dInfer Soft Token Experimentation Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Added dInfer to Python path: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python\n",
            "✓ Cleared cached dinfer modules\n",
            "✓ dInfer module found at: /lustre/fsw/portfolios/llmservice/users/mfathi/codebases/nemo-rl/3rdparty/dInfer/python/dinfer/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Add dInfer to Python path from the 3rdparty submodule\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add the dInfer python directory to sys.path\n",
        "DINFER_PATH = os.path.abspath('3rdparty/dInfer/python')\n",
        "if os.path.exists(DINFER_PATH):\n",
        "    if DINFER_PATH not in sys.path:\n",
        "        sys.path.insert(0, DINFER_PATH)\n",
        "    print(f\"✓ Added dInfer to Python path: {DINFER_PATH}\")\n",
        "else:\n",
        "    print(f\"⚠ Warning: dInfer path not found: {DINFER_PATH}\")\n",
        "    print(\"  Make sure you're running this notebook from the project root.\")\n",
        "\n",
        "# Clear any cached imports to ensure we use the latest code\n",
        "import importlib\n",
        "for module_name in list(sys.modules.keys()):\n",
        "    if 'dinfer' in module_name:\n",
        "        del sys.modules[module_name]\n",
        "print(\"✓ Cleared cached dinfer modules\")\n",
        "\n",
        "# Verify the import works\n",
        "try:\n",
        "    import dinfer\n",
        "    print(f\"✓ dInfer module found at: {dinfer.__file__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"✗ Failed to import dInfer: {e}\")\n",
        "\n",
        "# Note: You can also use the import utilities from xp/llada_api/llada_generate/dinfer/_imports.py\n",
        "# which provides the same functionality with additional error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/nemo_rl_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-24 06:33:17 [__init__.py:216] Automatically detected platform cuda.\n",
            "Torch compilation disabled to avoid backend issues\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import time\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Disable torch compilation to avoid backend compiler errors\n",
        "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
        "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
        "\n",
        "# Disable torch.compile globally\n",
        "torch._dynamo.config.disable = True\n",
        "\n",
        "# dinfer imports (using local dInfer from 3rdparty/dInfer/python added to path above)\n",
        "from dinfer.model import LLaDAModelLM\n",
        "from dinfer.decoding.parallel_strategy import (\n",
        "    ParallelDecoder,\n",
        "    ThresholdParallelDecoder,\n",
        "    CreditThresholdParallelDecoder,\n",
        "    HierarchyDecoder,\n",
        "    get_num_transfer_tokens,\n",
        "    get_transfer_index,\n",
        ")\n",
        "from dinfer import (\n",
        "    BlockWiseDiffusionLLM,\n",
        "    BlockIteratorFactory,\n",
        "    KVCacheFactory,\n",
        "    SlidingWindowDiffusionLLM,\n",
        ")\n",
        "\n",
        "from dinfer.decoding.utils import (\n",
        "    TokenArray,\n",
        ")\n",
        "\n",
        "print(\"Torch compilation disabled to avoid backend issues\")\n",
        "\n",
        "# LLaDA tokenizer constants\n",
        "MASK_ID = 126336\n",
        "EOS_ID = 126081\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from GSAI-ML/LLaDA-8B-Base...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 6 files: 100%|██████████| 6/6 [00:23<00:00,  3.95s/it]\n",
            "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00, 295.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model and tokenizer loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "MODEL_PATH = \"GSAI-ML/LLaDA-8B-Base\"  # Update this path to your model\n",
        "# MODEL_PATH = \"GSAI-ML/LLaDA-8B-Instruct\"  # Update this path to your model\n",
        "# MODEL_PATH = \"GSAI-ML/LLaDA-1.5\"\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model from {MODEL_PATH}...\")\n",
        "model = LLaDAModelLM.from_pretrained(\n",
        "    MODEL_PATH, \n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16, \n",
        "    init_device=str(device)  # Convert device to string for JSON serialization\n",
        ").eval()\n",
        "model = model.to(device)\n",
        "\n",
        "# Optional: Compile the model for better performance\n",
        "# model = torch.compile(model, mode='reduce-overhead', fullgraph=True)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
        "print(\"Model and tokenizer loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decoder and generation pipeline configured!\n"
          ]
        }
      ],
      "source": [
        "# Generation parameters - EXPERIMENT WITH THESE!\n",
        "generation_config = {\n",
        "    \"gen_length\": 256,      # Maximum number of tokens to generate\n",
        "    \"steps\": 64,\n",
        "    \"block_length\": 64,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": True,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "}\n",
        "\n",
        "# Create decoder with threshold strategy\n",
        "decoder = ThresholdParallelDecoder(0, threshold=generation_config[\"threshold\"], mask_id=MASK_ID, eos_id=EOS_ID)\n",
        "\n",
        "# Alternative: Use FixedParallelDecoderWithEOS for fixed-step decoding\n",
        "# decoder = FixedParallelDecoderWithEOS(\n",
        "#     temperature=0,\n",
        "#     steps=generation_config[\"steps\"],\n",
        "#     mask_id=MASK_ID,\n",
        "#     eos_id=EOS_ID\n",
        "# )\n",
        "# print(\"Using FixedParallelDecoderWithEOS\")\n",
        "\n",
        "# Create iterator factory\n",
        "iterator_factory = BlockIteratorFactory(True)\n",
        "\n",
        "# Create KV cache factory if using caching\n",
        "cache_factory = KVCacheFactory(generation_config[\"cache_type\"]) if generation_config[\"cache_type\"] else None\n",
        "\n",
        "# Create the Diffusion LLM instance\n",
        "dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        "    maximum_unroll=generation_config[\"maximum_unroll\"],\n",
        "    expected_tpf=generation_config[\"expected_tpf\"]\n",
        ")\n",
        "\n",
        "print(\"Decoder and generation pipeline configured!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt length: 46 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 242\n",
            "- Cache updates: 4\n",
            "- Time: 5.11s\n",
            "- Tokens/second: 50.14\n",
            "- Forwards/second: 47.40\n",
            "\n",
            "Generated text:\n",
            "<|startoftext|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Once upon a time in a magical forest<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "There was a little girl who lived in a small cottage in the forest. She was a kind and curious girl who loved to explore the woods. One day, she stumbled upon a mysterious cave that seemed to be hidden among the trees. She decided to enter the cave and see what was inside.\n",
            "\n",
            "As she entered the cave, she was greeted by a warm glow that seemed to emanate from the walls. She continued to explore the cave and found a small room with a large table in the center. The table was covered with books and papers, and the girl realized that she had stumbled upon a magical library.\n",
            "\n",
            "The girl spent hours reading through the books and learning about the secrets of the forest. She discovered that the forest was home to many magical creatures, such as unicorns, dragons, and fairies. She also learned that the forest was protected by a powerful magic that kept it safe from harm.\n",
            "\n",
            "The girl was amazed by what she had learned and decided to explore the forest more deeply. She encountered many magical creatures and learned about their powers and abilities. She also discovered that the forest was home to many hidden treasures, such as precious gems and magical artifacts.\n",
            "\n",
            "As the girl continued to explore the forest, she realized that she had found\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def generate_text(prompt, dllm_instance=dllm, tokenizer=tokenizer, config=generation_config, apply_chat_template=True):\n",
        "    \"\"\"\n",
        "    Generate text using the diffusion LLM.\n",
        "    \n",
        "    Args:\n",
        "        prompt: Input text prompt\n",
        "        dllm_instance: Diffusion LLM instance\n",
        "        tokenizer: Tokenizer instance\n",
        "        config: Generation configuration dict\n",
        "    \n",
        "    Returns:\n",
        "        Generated text string\n",
        "    \"\"\"\n",
        "    # Tokenize the prompt\n",
        "    if apply_chat_template:\n",
        "        message = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False)\n",
        "    else:\n",
        "        message = prompt\n",
        "    input_ids = tokenizer(message, return_tensors=\"pt\")['input_ids'].to(device)\n",
        "    prompt_length = input_ids.shape[1]\n",
        "    \n",
        "    print(f\"Prompt length: {prompt_length} tokens\")\n",
        "    print(f\"Generating up to {config['gen_length']} tokens...\")\n",
        "    \n",
        "    # Track statistics\n",
        "    prev_forwards = dllm_instance.num_forwards\n",
        "    prev_cache_updates = dllm_instance.cache_updates\n",
        "    \n",
        "    # Generate\n",
        "    start_time = time.time()\n",
        "    output_ids = dllm_instance.generate(\n",
        "        input_ids, \n",
        "        gen_length=config['gen_length'], \n",
        "        block_length=config['block_length']\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    \n",
        "    # Calculate statistics\n",
        "    total_forwards = dllm_instance.num_forwards - prev_forwards\n",
        "    total_cache_updates = dllm_instance.cache_updates - prev_cache_updates\n",
        "    generated_tokens = output_ids.shape[1] - prompt_length\n",
        "    generation_time = end_time - start_time\n",
        "    \n",
        "    # Decode output\n",
        "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "    \n",
        "    # Print statistics\n",
        "    print(f\"\\nGeneration Statistics:\")\n",
        "    print(f\"- Generated tokens: {generated_tokens}\")\n",
        "    print(f\"- Forward passes: {total_forwards}\")\n",
        "    print(f\"- Cache updates: {total_cache_updates}\")\n",
        "    print(f\"- Time: {generation_time:.2f}s\")\n",
        "    print(f\"- Tokens/second: {generated_tokens/generation_time:.2f}\")\n",
        "    print(f\"- Forwards/second: {total_forwards/generation_time:.2f}\")\n",
        "    \n",
        "    return generated_text\n",
        "\n",
        "# Test the generation function\n",
        "test_prompt = \"Once upon a time in a magical forest\"\n",
        "generated = generate_text(test_prompt, apply_chat_template=False)\n",
        "print(f\"\\nGenerated text:\\n{generated}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "PROMPT: The future of artificial intelligence is\n",
            "================================================================================\n",
            "Prompt length: 6 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 199\n",
            "- Cache updates: 4\n",
            "- Time: 3.74s\n",
            "- Tokens/second: 68.50\n",
            "- Forwards/second: 53.25\n",
            "\n",
            "GENERATED:\n",
            "The future of artificial intelligence is bright, and it is likely to continue to evolve and improve in the coming years. Here are some potential areas where AI could make significant contributions:\n",
            "1. Healthcare: AI has the potential to revolutionize healthcare by improving diagnosis, treatment, and patient outcomes. For example, AI could be used to analyze medical images and identify diseases earlier and more accurately than humans.\n",
            "2. Transportation: AI has the potential to improve transportation by reducing congestion, improving safety, and increasing efficiency. For example, AI could be used to optimize traffic flow, predict traffic patterns, and develop autonomous vehicles.\n",
            "3. Education: AI has the potential to personalize education and improve student outcomes. For example, AI could be used to develop adaptive learning systems that adjust to each student's learning style and pace.\n",
            "4. Finance: AI has the potential to improve finance by reducing fraud, managing risk, and improving investment decisions. For example, AI could be used to analyze financial data and identify patterns that could indicate fraudulent activity.\n",
            "5. Manufacturing: AI has the potential to improve manufacturing by reducing costs, improving quality, and increasing efficiency. For example, AI could be used to optimize production processes, predict equipment failures, and develop autonomous robots.\n",
            "Overall, AI has the potential to transform many industries\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Interactive prompt examples\n",
        "prompts = [\n",
        "    \"The future of artificial intelligence is\",\n",
        "    \"Explain quantum computing in simple terms:\",\n",
        "    \"Write a haiku about programming:\",\n",
        "    \"The most important scientific discovery was\",\n",
        "]\n",
        "\n",
        "# Generate for each prompt\n",
        "for prompt in prompts[:1]:  # Change to prompts to run all\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROMPT: {prompt}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    generated = generate_text(prompt, apply_chat_template=False)\n",
        "    print(f\"\\nGENERATED:\\n{generated}\")\n",
        "    print(f\"{'='*80}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.7\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 142\n",
            "- Cache updates: 4\n",
            "- Time: 2.66s\n",
            "- Tokens/second: 96.41\n",
            "- Forwards/second: 53.48\n",
            "\n",
            "Generated:  a complex and multifaceted question that has been debated by philosophers, scientists, and religious leaders for centuries. While there is no definitive answer to this question, there are several popular theories that have been proposed to explain the purpose of life.\n",
            "\n",
            "One theory suggests that the purpose of life is to seek happiness and fulfillment. This theory is often associated with the philosophy of hedonism, which holds that the ultimate goal of life is to seek pleasure and happiness. Advocates of this theory argue that the pursuit of happiness is the most important aspect of life, and that individuals should strive to achieve happiness through any means necessary.\n",
            "\n",
            "Another theory suggests that the purpose of life is to seek knowledge and understanding. This theory is often associated with the philosophy of intellectualism, which holds that the ultimate goal of life is to gain knowledge and understanding of the world. Advocates of this theory argue that the pursuit of knowledge is the most important aspect of life, and that individuals should strive to gain a deep understanding of the world and their place in it.\n",
            "\n",
            "A third theory suggests that the purpose of life is to serve others. This theory is often associated with the philosophy of altruism, which holds that the ultimate goal of life is to serve others and promote the greater good. Advocates of this theory\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.8\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 170\n",
            "- Cache updates: 4\n",
            "- Time: 3.18s\n",
            "- Tokens/second: 80.47\n",
            "- Forwards/second: 53.44\n",
            "\n",
            "Generated:  a complex and multifaceted question that has been debated by philosophers, scientists, and religious leaders for centuries. While there is no definitive answer to this question, there are several popular theories that have been proposed to explain the purpose of life.\n",
            "\n",
            "One theory suggests that the purpose of life is to seek happiness and fulfillment. This theory is often associated with the philosophy of hedonism, which holds that the ultimate goal of life is to seek pleasure and happiness. According to this theory, individuals should strive to experience as much pleasure as possible, regardless of the cost or the consequences.\n",
            "\n",
            "Another theory suggests that the purpose of life is to achieve meaning and purpose. This theory is often associated with the philosophy of existentialism, which holds that the ultimate goal of life is to achieve a sense of meaning and purpose. According to this theory, individuals should strive to find a sense of meaning and purpose in their lives, even if it means sacrificing pleasure and happiness.\n",
            "\n",
            "A third theory suggests that the purpose of life is to serve God and help others. This theory is often associated with the philosophy of altruism, which holds that the ultimate goal of life is to serve God and help others. According to this theory, individuals should strive to make a positive impact on the world and to\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.9\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 186\n",
            "- Cache updates: 4\n",
            "- Time: 3.48s\n",
            "- Tokens/second: 73.52\n",
            "- Forwards/second: 53.41\n",
            "\n",
            "Generated:  a complex and multifaceted question that has been debated by philosophers, scientists, and religious leaders for centuries. While there is no definitive answer to this question, there are several popular theories that have been proposed to explain the purpose of life.\n",
            "\n",
            "One theory suggests that the purpose of life is to seek happiness and fulfillment. This theory is often associated with the philosophy of hedonism, which holds that the ultimate goal of life is to seek pleasure and happiness. According to this theory, individuals should strive to experience as much pleasure as possible, regardless of the cost or the consequences.\n",
            "\n",
            "Another theory suggests that the purpose of life is to achieve meaning and purpose. This theory is often associated with the philosophy of existentialism, which holds that the ultimate goal of life is to achieve a sense of meaning and purpose. According to this theory, individuals should strive to find a sense of meaning and purpose in their lives, even if it means sacrificing pleasure and happiness.\n",
            "\n",
            "A third theory suggests that the purpose of life is to serve God and help others. This theory is often associated with the philosophy of altruism, which holds that the ultimate goal of life is to serve God and help others. According to this theory, individuals should strive to make a positive impact on the world and to\n",
            "\n",
            "================================================================================\n",
            "Testing with threshold = 0.95\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 196\n",
            "- Cache updates: 4\n",
            "- Time: 3.67s\n",
            "- Tokens/second: 69.71\n",
            "- Forwards/second: 53.37\n",
            "\n",
            "Generated:  a complex and multifaceted question that has been debated by philosophers, scientists, and religious leaders for centuries. While there is no definitive answer to this question, there are several popular theories that have been proposed to explain the purpose of life.\n",
            "\n",
            "One theory suggests that the purpose of life is to seek happiness and fulfillment. This theory is often associated with the philosophy of hedonism, which holds that the ultimate goal of life is to seek pleasure and happiness. According to this theory, individuals should strive to experience as much pleasure as possible, regardless of the cost or the consequences.\n",
            "\n",
            "Another theory suggests that the purpose of life is to achieve meaning and purpose. This theory is often associated with the philosophy of existentialism, which holds that the ultimate goal of life is to achieve a sense of meaning and purpose. According to this theory, individuals should strive to find a sense of meaning and purpose in their lives, even if it means sacrificing pleasure and happiness.\n",
            "\n",
            "A third theory suggests that the purpose of life is to serve God and help others. This theory is often associated with the philosophy of altruism, which holds that the ultimate goal of life is to serve God and help others. According to this theory, individuals should strive to make a positive impact on the world and to\n"
          ]
        }
      ],
      "source": [
        "# Experiment with different threshold values\n",
        "thresholds = [0.7, 0.8, 0.9, 0.95]\n",
        "test_prompt = \"The meaning of life is\"\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Testing with threshold = {threshold}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Create new decoder with different threshold\n",
        "    test_decoder = ThresholdParallelDecoder(0, threshold=threshold)\n",
        "    \n",
        "    # Create new DLLM instance\n",
        "    test_dllm = BlockWiseDiffusionLLM(\n",
        "        model=model,\n",
        "        decoder=test_decoder,\n",
        "        iterator_factory=iterator_factory,\n",
        "        cache_factory=cache_factory,\n",
        "        early_stop=True\n",
        "    )\n",
        "    \n",
        "    # Generate and compare\n",
        "    generated = generate_text(test_prompt, dllm_instance=test_dllm, apply_chat_template=False)\n",
        "    print(f\"\\nGenerated: {generated[len(test_prompt):]}\")  # Show only generated part\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Block-wise generation:\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 191\n",
            "- Forward passes: 176\n",
            "- Cache updates: 3\n",
            "- Time: 3.30s\n",
            "- Tokens/second: 57.95\n",
            "- Forwards/second: 53.40\n",
            "\n",
            "\n",
            "Sliding window generation:\n",
            "================================================================================\n",
            "Prompt length: 5 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 127\n",
            "- Forward passes: 121\n",
            "- Cache updates: 2\n",
            "- Time: 2.28s\n",
            "- Tokens/second: 55.78\n",
            "- Forwards/second: 53.14\n",
            "\n",
            "\n",
            "Comparison:\n",
            "Block-wise output:  the way we live and work. It will be more efficient, sustainable, and personalized. It will help us...\n",
            "Sliding window output:  the way we live and work. It will be more efficient, accurate, and personalized. It will enable us ...\n"
          ]
        }
      ],
      "source": [
        "# Create sliding window DLLM\n",
        "sliding_dllm = SlidingWindowDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=KVCacheFactory('dual'),  # Sliding window requires cache\n",
        "    prefix_look=0,      # How many tokens to look back\n",
        "    after_look=0,       # How many tokens to look ahead\n",
        "    warmup_steps=1,     # Warmup iterations\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Compare block-wise vs sliding window\n",
        "test_prompt = \"Artificial intelligence will revolutionize\"\n",
        "\n",
        "print(\"Block-wise generation:\")\n",
        "print(\"=\"*80)\n",
        "block_generated = generate_text(test_prompt, dllm_instance=dllm, apply_chat_template=False)\n",
        "\n",
        "print(\"\\n\\nSliding window generation:\")\n",
        "print(\"=\"*80)\n",
        "sliding_generated = generate_text(test_prompt, dllm_instance=sliding_dllm, apply_chat_template=False)\n",
        "\n",
        "print(\"\\n\\nComparison:\")\n",
        "print(f\"Block-wise output: {block_generated[len(test_prompt):][:100]}...\")\n",
        "print(f\"Sliding window output: {sliding_generated[len(test_prompt):][:100]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class _FixedParallelDecoder(ParallelDecoder):\n",
        "    \"\"\" This decoder decodes tokens in a fixed number of steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, temperature, steps, remasking='low_confidence', mask_id=MASK_ID, eos_id=EOS_ID):\n",
        "        super().__init__(temperature, remasking, mask_id)\n",
        "        self.steps = steps\n",
        "        self.iter = 0\n",
        "        self.eos_id = eos_id\n",
        "\n",
        "    def block_init(self, block_x, block_id):\n",
        "        # TODO(zhengda) we need to handle steps correctly here when the distributed version changes the gen length.\n",
        "        block_mask_index = block_x == self.mask_id\n",
        "        self.num_transfer_tokens = get_num_transfer_tokens(block_mask_index, self.steps)\n",
        "        self.iter = 0\n",
        "\n",
        "    def decode(self, logits, block_start, block_end, x, iter_threshold = None):\n",
        "        \"\"\" Decode the logits in a block.\n",
        "        \"\"\"\n",
        "        mask_index = (x[block_start:block_end] == self.mask_id)\n",
        "        assert mask_index.shape[1] == logits.shape[1]\n",
        "\n",
        "        curr_x = x[block_start:block_end]\n",
        "        x0, transfer_index = get_transfer_index(logits, self.temperature, self.remasking, mask_index, curr_x, self.num_transfer_tokens[:, self.iter], None)\n",
        "        self.iter += 1\n",
        "        x[block_start:block_end][transfer_index] = x0[transfer_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing FixedParallelDecoder with different fixed ratios...\n",
            "Prompt length: 7 tokens\n",
            "Generating up to 256 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 256\n",
            "- Forward passes: 256\n",
            "- Cache updates: 4\n",
            "- Time: 4.90s\n",
            "- Tokens/second: 52.22\n",
            "- Forwards/second: 52.22\n",
            "\n",
            "Generated: \n",
            "\n",
            "The key to innovation is to be open to new ideas and perspectives, to be willing to take risks and to be willing to learn from failure.\n",
            "\n",
            "What is the key to success?\n",
            "\n",
            "The key to success is to be persistent, to stay focused on your goals and to be willing to work hard.\n",
            "\n",
            "What is the key to happiness?\n",
            "\n",
            "The key to happiness is to be grateful, to find joy in the moment and to be kind to yourself and others.\n",
            "\n",
            "What is the key to leadership?\n",
            "\n",
            "The key to leadership is to be authentic, to be transparent and to be willing to listen to others.\n",
            "\n",
            "What is the key to trust?\n",
            "\n",
            "The key to trust is to be honest, to be reliable and to be willing to listen to others.\n",
            "\n",
            "What is the key to love?\n",
            "\n",
            "The key to love is to be patient, to be understanding and to be willing to listen to others.\n",
            "\n",
            "What is the key to friendship?\n",
            "\n",
            "The key to friendship is to be loyal, to be supportive and to be willing to listen to others.\n",
            "\n",
            "What is the key to family?\n",
            "\n",
            "The key to family is to be present, to be supportive and to be willing to\n"
          ]
        }
      ],
      "source": [
        "# Experiment with FixedParallelDecoder if available\n",
        "print(\"Testing FixedParallelDecoder with different fixed ratios...\")\n",
        "\n",
        "test_prompt = \"What is the key to innovation?\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "# Create DLLM with fixed decoder\n",
        "fixed_dllm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=True\n",
        ")\n",
        "\n",
        "# Generate\n",
        "generated = generate_text(test_prompt, dllm_instance=fixed_dllm, apply_chat_template=False)\n",
        "print(f\"\\nGenerated: {generated[len(test_prompt):]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft Token Diffusion Sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BlockWiseSoftTokenLLM:\n",
        "    \"\"\"\n",
        "    Block-wise diffusion LLM with Soft Token Sampling.\n",
        "    Adapted from BlockWiseSoftTokenLLM in soft_token_experiment.py.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, decoder, iterator_factory, early_stop=True, cache_factory=None, maximum_unroll=4, expected_tpf=8, soft_token_ratio=0.2, treat_soft_tokens_as_candidates=False, soft_temperature=1.0):\n",
        "        self.model = model\n",
        "        self.cache_factory = cache_factory\n",
        "        self.decoder = decoder\n",
        "        self.iterator_factory = iterator_factory\n",
        "        self.num_forwards = 0\n",
        "        self.cache_updates = 0\n",
        "        self.early_stop = early_stop\n",
        "        self.maximum_unroll = maximum_unroll\n",
        "        self.expected_tpf = expected_tpf\n",
        "        self.soft_token_ratio = soft_token_ratio\n",
        "        self.treat_soft_tokens_as_candidates = treat_soft_tokens_as_candidates\n",
        "        self.soft_temperature = soft_temperature\n",
        "        self.input_embeddings = self.model.get_input_embeddings()\n",
        "\n",
        "    def _compute_logits(self, x, block_loc, kv_cache, use_input_embeds=None):\n",
        "        \"\"\"Helper to run model with correct context and embeddings.\"\"\"\n",
        "        # Determine input context based on cache type\n",
        "        if kv_cache is None:\n",
        "            # Full context (no cache)\n",
        "            if use_input_embeds is not None:\n",
        "                logits = self.model(inputs_embeds=use_input_embeds).logits\n",
        "            else:\n",
        "                logits = self.model(x.data).logits\n",
        "            return logits[:, block_loc.start:block_loc.end]\n",
        "            \n",
        "        elif kv_cache.cache_type == 'prefix':\n",
        "            # Prefix Cache: past_key_values contains context up to block_start\n",
        "            past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "            \n",
        "            if use_input_embeds is not None:\n",
        "                # Input embeddings should correspond to x[block_loc.start:]\n",
        "                logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            else:\n",
        "                logits = self.model(x[block_loc.start:], past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            \n",
        "            curr_len = block_loc.end - block_loc.start\n",
        "            return logits[:, :curr_len]\n",
        "\n",
        "        else:\n",
        "            # Dual/Sliding Cache: typically uses block context\n",
        "            past_key_values, replace_position = kv_cache.get_key_values(block_loc.start, block_loc.end)\n",
        "            \n",
        "            if use_input_embeds is not None:\n",
        "                 logits = self.model(inputs_embeds=use_input_embeds, past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            else:\n",
        "                 # Use x slice instead of block to ensure we have the latest updates\n",
        "                 logits = self.model(x[block_loc.start:block_loc.end], past_key_values=past_key_values, use_cache=True,\n",
        "                                  replace_position=replace_position).logits\n",
        "            return logits\n",
        "\n",
        "    def validate_schedule(self, block_length, soft_token_ratio, treat_soft_tokens_as_candidates):\n",
        "        \"\"\" Validates that the decoding schedule can be satisfied with the given soft token ratio.\n",
        "        \"\"\"\n",
        "        # Only validate for FixedParallelDecoder which has steps\n",
        "        if not hasattr(self.decoder, 'steps') or treat_soft_tokens_as_candidates:\n",
        "            return\n",
        "\n",
        "        steps = self.decoder.steps\n",
        "        current_masks = block_length\n",
        "        \n",
        "        # Calculate the schedule for a full block\n",
        "        base = current_masks // steps\n",
        "        remainder = current_masks % steps\n",
        "        \n",
        "        schedule = []\n",
        "        for i in range(steps):\n",
        "            count = base + (1 if i < remainder else 0)\n",
        "            schedule.append(count)\n",
        "            \n",
        "        # Simulate decoding\n",
        "        for step_idx, num_to_decode in enumerate(schedule):\n",
        "            num_soft = int(current_masks * soft_token_ratio)\n",
        "            available = current_masks - num_soft\n",
        "            \n",
        "            if available < num_to_decode:\n",
        "                # Just warn instead of raising error to prevent crashing server\n",
        "                print(\n",
        "                    f\"Decoding Schedule Violation: Step {step_idx} requires decoding {num_to_decode} tokens, \"\n",
        "                    f\"but only {available} masks are available ({current_masks} total - {num_soft} soft tokens). \"\n",
        "                    f\"Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\"\n",
        "                )\n",
        "                return\n",
        "            current_masks -= num_to_decode\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, gen_length=128, block_length=128, soft_token_ratio=None, treat_soft_tokens_as_candidates=None, steps=None, threshold=None, soft_temperature=None):\n",
        "        ''' Generate tokens with diffusion iterations block by block using Soft Token Sampling.\n",
        "        '''\n",
        "        # Use instance defaults if not provided\n",
        "        if soft_token_ratio is None:\n",
        "            soft_token_ratio = self.soft_token_ratio\n",
        "        if treat_soft_tokens_as_candidates is None:\n",
        "            treat_soft_tokens_as_candidates = self.treat_soft_tokens_as_candidates\n",
        "        if soft_temperature is None:\n",
        "            soft_temperature = self.soft_temperature\n",
        "            \n",
        "        # Update decoder parameters\n",
        "        if steps is not None and hasattr(self.decoder, 'steps'):\n",
        "            self.decoder.steps = steps\n",
        "            \n",
        "        if threshold is not None and hasattr(self.decoder, 'threshold'):\n",
        "            self.decoder.threshold = threshold\n",
        "            \n",
        "        self.validate_schedule(block_length, soft_token_ratio, treat_soft_tokens_as_candidates)\n",
        "\n",
        "        x = TokenArray(prompt, gen_length, self.decoder.mask_id, self.decoder.eos_id, self.model.device)\n",
        "        it = self.iterator_factory.create(x, block_length)\n",
        "\n",
        "        iter_no = 0\n",
        "        kv_cache = self.cache_factory.create() if self.cache_factory is not None else None\n",
        "        \n",
        "        for block_id, (block_loc, block) in enumerate(it):\n",
        "            self.decoder.block_init(block, block_id)\n",
        "            \n",
        "            while (block == self.decoder.mask_id).sum() > 0:\n",
        "                \n",
        "                # Calculate unroll_k based on mask count and expected TPF\n",
        "                unroll_k = max(min((block == self.decoder.mask_id).sum()//self.expected_tpf, self.maximum_unroll), 1)\n",
        "                \n",
        "                for unroll_i in range(unroll_k):\n",
        "                    # Pre-check: Ensure we can satisfy the soft token ratio without violating the decoding schedule\n",
        "                    # if we choose to exclude soft tokens from candidacy.\n",
        "                    current_masks = (x[block_loc.start:block_loc.end] == self.decoder.mask_id).sum().item()\n",
        "                    \n",
        "                    # Optimization: If no masks left, stop unrolling (matches blockwise behavior)\n",
        "                    if current_masks == 0:\n",
        "                        break\n",
        "\n",
        "                    num_soft = int(current_masks * soft_token_ratio)\n",
        "                    \n",
        "                    # Determine num_to_decode for the current step\n",
        "                    num_to_decode = 0\n",
        "                    if hasattr(self.decoder, 'num_transfer_tokens'):\n",
        "                        # Fixed schedule\n",
        "                        if self.decoder.iter < self.decoder.num_transfer_tokens.shape[1]:\n",
        "                            num_to_decode = self.decoder.num_transfer_tokens[0, self.decoder.iter].item()\n",
        "                    else:\n",
        "                        # Dynamic schedule (Threshold decoder) - estimation not straightforward here without logits\n",
        "                        pass\n",
        "                    \n",
        "                    if not treat_soft_tokens_as_candidates and num_to_decode > 0:\n",
        "                        # If soft tokens CANNOT be decoded, we must have enough pure masks left to satisfy decoder demand\n",
        "                        available_for_decoding = current_masks - num_soft\n",
        "                        if available_for_decoding < num_to_decode:\n",
        "                            # Log warning instead of crashing\n",
        "                            print(\n",
        "                                f\"Decoding Schedule Violation: Step {self.decoder.iter} requires decoding {num_to_decode} tokens, \"\n",
        "                                f\"but only {available_for_decoding} masks are available ({current_masks} total - {num_soft} soft tokens). \"\n",
        "                                f\"Reduce soft_token_ratio or enable treat_soft_tokens_as_candidates.\"\n",
        "                            )\n",
        "                            # Adjust num_soft to make it work\n",
        "                            num_soft = max(0, current_masks - num_to_decode)\n",
        "\n",
        "                    # 1. Handle KV Cache Update (Initial step for block or periodically)\n",
        "                    if kv_cache is not None and kv_cache.require_update(iter_no, block_loc.start, block_loc.end):\n",
        "                        output = self.model(x.data, use_cache=True)\n",
        "                        self.num_forwards += 1\n",
        "                        \n",
        "                        # Update cache\n",
        "                        kv_cache.update(output.past_key_values)\n",
        "                        self.cache_updates += 1\n",
        "                        \n",
        "                        # Decode using these initial logits (Standard dInfer behavior)\n",
        "                        self.decoder.decode(output.logits[:, block_loc.start:block_loc.end], block_loc.start, block_loc.end, x)\n",
        "\n",
        "                    # 2. Pass 1: Standard Logits (with current masks)\n",
        "                    logits1 = self._compute_logits(x, block_loc, kv_cache, use_input_embeds=None)\n",
        "                    self.num_forwards += 1\n",
        "                    \n",
        "                    decoding_logits = logits1\n",
        "                    soft_indices = None\n",
        "                    \n",
        "                    # 3. Soft Token Logic\n",
        "                    # Identify masks in the current block\n",
        "                    curr_block_ids = x[block_loc.start:block_loc.end]\n",
        "                    mask_mask = (curr_block_ids == self.decoder.mask_id)\n",
        "                    mask_indices = torch.nonzero(mask_mask).flatten() # Indices relative to block start\n",
        "                    \n",
        "                    if mask_indices.numel() > 0 and soft_token_ratio > 0:\n",
        "                        if num_soft > 0:\n",
        "                            perm = torch.randperm(mask_indices.numel(), device=self.model.device)\n",
        "                            soft_indices = mask_indices[perm[:num_soft]] # Indices relative to block start\n",
        "                            \n",
        "                            # Extract logits for these positions\n",
        "                            # logits1 shape: [1, block_len, vocab]\n",
        "                            selected_logits = logits1[0, soft_indices]\n",
        "                            \n",
        "                            # Apply soft temperature\n",
        "                            if soft_temperature > 0:\n",
        "                                selected_logits = selected_logits / soft_temperature\n",
        "\n",
        "                            probs = torch.softmax(selected_logits, dim=-1)\n",
        "                            \n",
        "                            # Compute Soft Embeddings: Weighted average of token embeddings\n",
        "                            # [num_soft, vocab] @ [vocab, d_model] -> [num_soft, d_model]\n",
        "                            soft_embeds = torch.matmul(probs, self.input_embeddings.weight)\n",
        "                            \n",
        "                            # Prepare Input Embeddings\n",
        "                            target_ids = None\n",
        "                            global_offset = 0\n",
        "                            \n",
        "                            if kv_cache is None:\n",
        "                                target_ids = x.data\n",
        "                                global_offset = block_loc.start # Offset in target_ids\n",
        "                            elif kv_cache.cache_type == 'prefix':\n",
        "                                target_ids = x[block_loc.start:]\n",
        "                                global_offset = 0 # relative to start of target_ids\n",
        "                            else:\n",
        "                                target_ids = curr_block_ids\n",
        "                                global_offset = 0\n",
        "                            \n",
        "                            # Get base embeddings for the input context\n",
        "                            inputs_embeds = self.input_embeddings(target_ids).clone() # [1, len, d_model]\n",
        "                            \n",
        "                            # Replace masks with soft embeddings\n",
        "                            inputs_embeds[0, global_offset + soft_indices] = soft_embeds\n",
        "                            \n",
        "                            # Pass 2: Get logits with Soft Tokens\n",
        "                            logits2 = self._compute_logits(x, block_loc, kv_cache, use_input_embeds=inputs_embeds)\n",
        "                            self.num_forwards += 1\n",
        "                            decoding_logits = logits2\n",
        "\n",
        "\n",
        "                    # Force EOS probability to zero (effectively) to prevent soft token averaging from including EOS\n",
        "                    # if hasattr(self.decoder, 'eos_id'):\n",
        "                    #     decoding_logits[:, :, self.decoder.eos_id] = -10000.0\n",
        "\n",
        "                    # 4. Decode using the latest logits\n",
        "                    if not treat_soft_tokens_as_candidates and soft_indices is not None and soft_indices.numel() > 0:\n",
        "                        # We want to prevent these indices from being selected.\n",
        "                        # Set logits for soft tokens to a uniform distribution (max entropy -> min confidence)\n",
        "                        decoding_logits_modified = decoding_logits.clone()\n",
        "                        decoding_logits_modified[0, soft_indices] = 0.1 \n",
        "                        \n",
        "                        self.decoder.decode(decoding_logits_modified, block_loc.start, block_loc.end, x)\n",
        "                    else:\n",
        "                        self.decoder.decode(decoding_logits, block_loc.start, block_loc.end, x)\n",
        "                        \n",
        "                    iter_no += 1\n",
        "\n",
        "            # Early stop at EOS\n",
        "            if self.early_stop and torch.any(x[block_loc.start:block_loc.end] == self.decoder.eos_id):\n",
        "                x[block_loc.end:] = self.decoder.eos_id\n",
        "                break\n",
        "\n",
        "        # DEBUG: Check for EOS tokens to explain short output\n",
        "        eos_count = (x.data == self.decoder.eos_id).sum().item()\n",
        "        if eos_count > 0:\n",
        "            total_len = x.total_length\n",
        "            print(f\"SoftTokenLLM Generated {eos_count} EOS tokens out of {total_len} total positions. \"\n",
        "                  f\"This will shorten the output by {eos_count} tokens.\")\n",
        "                           \n",
        "        return x.get_generated_tokens()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating with Soft Token LLM...\n",
            "Prompt length: 12 tokens\n",
            "Generating up to 512 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 512\n",
            "- Forward passes: 1008\n",
            "- Cache updates: 8\n",
            "- Time: 25.84s\n",
            "- Tokens/second: 19.81\n",
            "- Forwards/second: 39.00\n",
            "\n",
            "Generated: On the theory of relativity, the relationship between energy and mass is:\n",
            "\n",
            "$$E = mc^2$$\n",
            "\n",
            "where $E$ is the energy, $m$ is the mass, and $c$ is the speed of light.\n",
            "\n",
            "Given that the speed of light is $c = 3 \\times 10^8$ meters per second, calculate the energy released by a particle with a mass of 1 gram. Express your answer in joules.\n",
            "\n",
            "## Solution\n",
            "\n",
            "To solve this problem, we will use the formula for energy in terms of mass:\n",
            "\n",
            "$$E = mc^2$$\n",
            "\n",
            "Given:\n",
            "- Mass $m = 1$ gram\n",
            "- Speed of light $c = 3 \\times 10^8$ meters per second\n",
            "\n",
            "First, we need to convert the mass from grams to kilograms since the speed of light is given in meters per second. We know that 1 gram is equal to 0.001 kilograms.\n",
            "\n",
            "So,\n",
            "\n",
            "$$m = 0.001 \\text{ kilograms}$$\n",
            "\n",
            "Now, we can substitute the values into the formula:\n",
            "\n",
            "$$E = (0.001 \\text{ kg}) \\times (3 \\times 10^8 \\text{ m/s})^2$$\n",
            "\n",
            "Calculate the square of the speed of light:\n",
            "\n",
            "$$c^2 = (3 \\times 10^8 \\text{ m/s})^2 = 9 \\times 10^{16} \\text{ m}^2/\\text{s}^2$$\n",
            "\n",
            "Now, multiply the mass by the squared speed:\n",
            "\n",
            "$$E = 0.001 \\text{ kg} \\times 9 \\times 10^{16} \\text{ m}^2/\\text{s}^2$$\n",
            "\n",
            "$$E = 0.001 \\times 9 \\times 10^{16} \\text{ joules}$$\n",
            "\n",
            "$$E = 9 \\times 10^{13} \\text{ joules}$$\n",
            "\n",
            "Therefore, the energy released by a particle with a mass of 1 gram is $9 \\times 10^{13}$ joules.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "The energy released by a particle with a mass of 1 gram is $9 \\times 10^{13}$ joules. This demonstrates the tremendous amount of energy that can be derived from a small\n",
            "--------------------------------\n",
            "Generating with Block Wise LLM...\n",
            "Prompt length: 12 tokens\n",
            "Generating up to 512 tokens...\n",
            "\n",
            "Generation Statistics:\n",
            "- Generated tokens: 511\n",
            "- Forward passes: 512\n",
            "- Cache updates: 8\n",
            "- Time: 13.13s\n",
            "- Tokens/second: 38.92\n",
            "- Forwards/second: 39.00\n",
            "\n",
            "Generated: On the theory of relativity, the relationship between energy and mass is given by Einstein's famous equation:\n",
            "\n",
            "$$ E = mc^2 $$\n",
            "\n",
            "where \\( E \\) is the energy, \\( m \\) is the mass, and \\( c \\) is the speed of light.\n",
            "\n",
            "Given that the speed of light \\( c \\) is approximately \\( 3 \\times 10^8 \\) meters per second, calculate the energy equivalent of a 1-kilogram mass.\n",
            "\n",
            "## Solution\n",
            "\n",
            "To find the energy equivalent of a 1-kilogram mass, we use Einstein's equation:\n",
            "\n",
            "$$ E = mc^2 $$\n",
            "\n",
            "Given:\n",
            "- Mass \\( m = 1 \\) kilogram\n",
            "- Speed of light \\( c = 3 \\times 10^8 \\) meters per second\n",
            "\n",
            "Substitute these values into the equation:\n",
            "\n",
            "$$ E = (1 \\, \\text{kg}) \\times (3 \\times 10^8 \\, \\text{m/s})^2 $$\n",
            "\n",
            "First, calculate the square of the speed of light:\n",
            "\n",
            "$$ c^2 = (3 \\times 10^8 \\, \\text{m/s})^2 = 9 \\times 10^{16} \\, \\text{m}^2/\\text{s}^2 $$\n",
            "\n",
            "Now, multiply the mass by the squared speed of light:\n",
            "\n",
            "$$ E = 1 \\, \\text{kg} \\times 9 \\times 10^{16} \\, \\text{m}^2/\\text{s}^2 $$\n",
            "\n",
            "$$ E = 9 \\times 10^{16} \\, \\text{Joules} $$\n",
            "\n",
            "Therefore, the energy equivalent of a 1-kilogram mass is:\n",
            "\n",
            "$$ E = 9 \\times 10^{16} \\, \\text{Joules} $$\n",
            "\n",
            "This calculation shows that a 1-kilogram mass is equivalent to \\( 9 \\times 10^{16} \\) Joules of energy according to Einstein's theory of relativity. This demonstrates the profound relationship between mass and energy in the context of relativity.## Question\n",
            "\n",
            "Consider a sequence of numbers defined by the following rule: Start with any positive integer \\( n \\). If \\( n \\) is even, divide it by 2. If \\( n \\) is odd, multiply it by 3 and add 1. This process is known as the Collatz conjecture or the \"hailstone sequence.\"\n",
            "\n",
            "Given this sequence, determine the\n"
          ]
        }
      ],
      "source": [
        "generation_config = {\n",
        "    \"gen_length\": 512,      # Maximum number of tokens to generate\n",
        "    \"steps\": 64,\n",
        "    \"block_length\": 64,     # Block size for parallel decoding\n",
        "    \"threshold\": 0.9,       # Confidence threshold for token acceptance\n",
        "    \"cache_type\": \"dual\",   # Options: None, \"prefix\", \"dual\"\n",
        "    \"early_stop\": False,     # Stop at EOS token\n",
        "    \"maximum_unroll\": 4,    # Maximum unroll steps\n",
        "    \"expected_tpf\": 8,      # Expected tokens per forward pass\n",
        "    \"treat_soft_tokens_as_candidates\": False,\n",
        "    \"soft_temperature\": 0.8,\n",
        "    \"soft_token_ratio\": 0.5,\n",
        "}\n",
        "\n",
        "test_prompt = \"On the theory of relativity, the relationship between energy and mass\"\n",
        "\n",
        "# Create FixedParallelDecoder\n",
        "fixed_decoder = _FixedParallelDecoder(\n",
        "    0, steps=generation_config[\"steps\"]\n",
        ")\n",
        "\n",
        "soft_llm = BlockWiseSoftTokenLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    soft_token_ratio=generation_config[\"soft_token_ratio\"],\n",
        "    treat_soft_tokens_as_candidates=generation_config[\"treat_soft_tokens_as_candidates\"],\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        "    soft_temperature=generation_config[\"soft_temperature\"],\n",
        ")\n",
        "\n",
        "block_wise_llm = BlockWiseDiffusionLLM(\n",
        "    model=model,\n",
        "    decoder=fixed_decoder,\n",
        "    iterator_factory=iterator_factory,\n",
        "    cache_factory=cache_factory,\n",
        "    early_stop=generation_config[\"early_stop\"],\n",
        ")\n",
        "\n",
        "# Generate\n",
        "print(\"Generating with Soft Token LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=soft_llm, config=generation_config, apply_chat_template=False)\n",
        "print(f\"\\nGenerated: {generated}\")\n",
        "\n",
        "print(\"--------------------------------\")\n",
        "\n",
        "print(\"Generating with Block Wise LLM...\")\n",
        "generated = generate_text(test_prompt, dllm_instance=block_wise_llm, config=generation_config, apply_chat_template=False)\n",
        "print(f\"\\nGenerated: {generated}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "SLURM Job Kernel (root)",
      "language": "python",
      "name": "slurm-job-kernel-mfathi"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
