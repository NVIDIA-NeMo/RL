defaults: "grpo_dapo17k_bytedtsinghua_qwen3_4binstruct_nf.yaml"

grpo:
  val_period: 1
  num_prompts_per_step: 1024
  num_generations_per_prompt: 64

policy:
  # Here, we use the Deepseek R1 train efficiency batching optimization
  # We generate one massive batch of rollouts and then take up to 16 steps off policy in one "grpo iteration"
  # Our base batch size is grpo.num_prompts_per_step = 64 and grpo.num_generations_per_prompt = 64, which yields a total of 4096 prompts.
  # That's why the train_global_batch_size is 4096.
  # But since we want to take up to 16 steps off policy, our actual grpo.num_prompts_per_step must be set to 16 times higher than it actually is
  # So the input grpo.num_prompts_per_step = 64 * 16 = 1024.
  train_global_batch_size: 4096

checkpointing:
  # Since we are doing larger batch rollouts, it's even more important that we stay efficient.
  # Normally we don't need to set this since we checkpoint every step, but now we are checkpointing only every "16 steps".
  checkpoint_must_save_by: "00:03:45:00"
