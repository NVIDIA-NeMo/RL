# GRPO Algorithm Configuration
defaults: "base.yaml"

grpo:
  num_prompts_per_step: 8
  num_generations_per_prompt: 8
  num_steps: 100
  normalize_rewards: true
  use_leave_one_out_baseline: true
  val_period: 10
  val_at_start: true
  max_val_samples: 16
  val_batch_size: 16

loss_fn:
  reference_policy_kl_penalty: 0.01
  ratio_eps: 0.2

checkpointing:
  enabled: true
  checkpoint_dir: "results/grpo"
  metric_name: "val_reward"
  higher_is_better: true
  keep_top_k: 3
  save_period: 10

policy:
  model_name: "meta-llama/Llama-3.2-1B-Instruct"
  train_global_batch_size: 32
  train_micro_batch_size: 4
  generation_batch_size: 32
  logprob_batch_size: 4
  max_total_sequence_length: 1024

  generation:
    backend: "vllm" # "vllm" or "hf"(to use the hf training framework's generation)
    max_new_tokens: ${policy.max_total_sequence_length} # upper bound, real truncation occurs at vllm.max_model_len below
    temperature: 1.0
    # Don't change since vllm logprobs in V0 runtime are after sampling and in V1 runtime are before sampling.
    top_p: 1.0
    top_k: null # disable
    vllm_cfg:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.7
      max_model_len: ${policy.max_total_sequence_length}

data:
  max_input_seq_length: ${policy.max_total_sequence_length} # upper bound, real truncation occurs at vllm.max_model_len
  prompt_file: "examples/prompts/cot.txt"
  system_prompt_file: null
  dataset_name: "datasets/Eurus-2-RL-Data/Eurus-2-RL-Data-math_train.jsonl"
  val_dataset_name: "datasets/Eurus-2-RL-Data/Eurus-2-RL-Data-math_val.jsonl"

env:
  math:
    num_workers: 8
