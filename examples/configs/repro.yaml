defaults: distillation_math.yaml
distillation:
  num_prompts_per_step: 512
  max_num_steps: 500
  val_batch_size: 512
  val_period: 20
loss_fn:
  kl_type: reverse
checkpointing:
  model_save_format: "torch_save"
  keep_top_k: 3
  checkpoint_dir: checkpoints/distillation-qwen3-32b-to-4b-base-long
policy:
  model_name: Qwen/Qwen3-4B-Base
  train_global_batch_size: 512
  max_total_sequence_length: 20480
  generation:
    vllm_cfg:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.7
teacher:
  model_name: Qwen/Qwen3-32B
  max_total_sequence_length: 20480
logger:
  log_dir: logs/distillation-qwen3-32b-to-4b-base-long
  wandb:
    project: nemo-rl
    name: distillation-qwen3-32b-to-4b-base-long
cluster:
  num_nodes: 2