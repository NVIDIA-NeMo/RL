# Math evaluation Configuration
defaults: "gpqa_eval.yaml"

data:
  prompt_file: "examples/prompts/humaneval.txt"
  dataset_name: "humaneval"

generation:
  vllm_cfg:
    max_model_len: 4096

env:
  math:
    verifier_type: "code"
    verifier_metadata_key: "tests"
