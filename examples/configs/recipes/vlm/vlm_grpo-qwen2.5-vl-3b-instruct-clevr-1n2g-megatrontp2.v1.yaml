defaults: ../../vlm_grpo_3B.yaml
checkpointing:
  checkpoint_dir: results/clevr_grpo
policy:
  max_total_sequence_length: 3072
  dtensor_cfg:
    enabled: false
  dynamic_batching:
    enabled: false
  make_sequence_length_divisible_by: ${policy.megatron_cfg.tensor_model_parallel_size}
  sequence_packing:
    train_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.train_micro_batch_size}}
    logprob_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.logprob_batch_size}}
    algorithm: modified_first_fit_decreasing
    sequence_length_round: 64
  optimizer: null
  generation:
    vllm_cfg:
      enable_expert_parallel: false
  megatron_cfg:
    enabled: true
    empty_unused_memory_level: 1
    activation_checkpointing: false
    converter_type: Qwen2ForCausalLM
    tensor_model_parallel_size: 1
    expert_tensor_parallel_size: 1
    expert_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    num_layers_in_first_pipeline_stage: null
    num_layers_in_last_pipeline_stage: null
    context_parallel_size: 1
    pipeline_dtype: ${policy.precision}
    sequence_parallel: false
    freeze_moe_router: true
    moe_router_dtype: fp64
    moe_router_load_balancing_type: none
    moe_router_bias_update_rate: 0.0
    moe_permute_fusion: false
    apply_rope_fusion: true
    optimizer:
      optimizer: adam
      lr: 5.0e-07
      min_lr: 5.0e-08
      weight_decay: 0.01
      bf16: true
      fp16: false
      params_dtype: float32
      adam_beta1: 0.9
      adam_beta2: 0.999
      adam_eps: 1.0e-08
      sgd_momentum: 0.9
      use_distributed_optimizer: true
      use_precision_aware_optimizer: true
      clip_grad: ${policy.max_grad_norm}
    scheduler:
      start_weight_decay: ${policy.megatron_cfg.optimizer.weight_decay}
      end_weight_decay: ${policy.megatron_cfg.optimizer.weight_decay}
      weight_decay_incr_style: constant
      lr_decay_style: constant
      lr_decay_iters: 1000
      lr_warmup_iters: 50
      lr_warmup_init: 5.0e-08
    distributed_data_parallel_config:
      grad_reduce_in_fp32: false
      overlap_grad_reduce: false
      overlap_param_gather: true
      average_in_collective: true
      use_custom_fsdp: false
      data_parallel_sharding_strategy: optim_grads_params
logger:
  wandb:
    name: vlm-grpo-3b-megatron
