defaults: ../../grpo_math_1B.yaml
grpo:
  num_prompts_per_step: 64
  num_generations_per_prompt: 32
loss_fn:
  use_importance_sampling_correction: true
policy:
  model_name: openai/gpt-oss-20b
  train_global_batch_size: 512
  train_micro_batch_size: 1
  generation_batch_size: 32 # Only used when generating using megatron backend
  logprob_batch_size: 4
  max_total_sequence_length: 4096
  megatron_cfg:
    enabled: true
    expert_model_parallel_size: 8
    tensor_model_parallel_size: 2
    sequence_parallel: true
    moe_permute_fusion: true
  dtensor_cfg:
    enabled: false
  dynamic_batching:
    enabled: true
  sequence_packing:
    enabled: false
cluster:
  num_nodes: 8
  num_gpus_per_node: 8
