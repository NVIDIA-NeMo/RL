defaults:
  - ../../grpo_math_1B.yaml
  - grpo-acereason-math-7b-16K.yaml
loss_fn:
  ratio_clip_c: 3
  reference_policy_kl_penalty: 0.0001
policy:
  max_total_sequence_length: 24576
  logprob_batch_size: 2
  dtensor_cfg:
    activation_checkpointing: true
    context_parallel_size: 2
  dynamic_batching:
    logprob_mb_tokens: 49152
    train_mb_tokens: 24576
  sequence_packing:
    enabled: false
    logprob_mb_tokens: 49152
    train_mb_tokens: 24576
  optimizer:
    kwargs:
      lr: 5.0e-07
  generation:
    max_new_tokens: 24576
    vllm_cfg:
      max_model_len: 24576
      gpu_memory_utilization: 0.8
      enforce_eager: true
data:
  max_input_seq_length: 24576

