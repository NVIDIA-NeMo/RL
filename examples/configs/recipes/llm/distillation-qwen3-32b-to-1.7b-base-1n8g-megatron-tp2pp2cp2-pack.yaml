defaults: ../../distillation_math.yaml
distillation:
  num_prompts_per_step: 32
  max_num_steps: 20
  val_batch_size: 32
  val_period: 10
  max_val_samples: 256
loss_fn:
  kl_type: reverse
checkpointing:
  checkpoint_dir: checkpoints/distillation-qwen3-32b-to-1.7b-base-megatron-tp2pp2cp2-pack
policy:
  train_global_batch_size: 32
  generation_batch_size: 32
  dtensor_cfg:
    enabled: false
  dynamic_batching:
    enabled: false
  sequence_packing:
    enabled: true
  make_sequence_length_divisible_by: ${mul:${mul:${.megatron_cfg.tensor_model_parallel_size},
    ${.megatron_cfg.context_parallel_size}}, 2}
  megatron_cfg:
    enabled: true
teacher:
  model_name: Qwen/Qwen3-32B
  dtensor_cfg:
    enabled: false
  dynamic_batching:
    enabled: false
  sequence_packing:
    enabled: true
  megatron_cfg:
    enabled: true
    tensor_model_parallel_size: 4
    context_parallel_size: 1
logger:
  log_dir: logs/distillation-qwen3-32b-to-1.7b-base-megatron-tp2pp2cp2-pack
  wandb:
    project: nemo-rl
    name: distillation-qwen3-32b-to-1.7b-base-megatron-tp2pp2cp2-pack
