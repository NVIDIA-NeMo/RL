# GRPO Algorithm Configuration
defaults: "grpo_math_1B.yaml"

grpo:
  num_prompts_per_step: 32
  num_generations_per_prompt: 16
  max_num_epochs: 1
  max_num_steps: 1000000
  normalize_rewards: true
  use_leave_one_out_baseline: true
  val_period: 10
  val_at_start: false
  val_at_end: false
  max_val_samples: 256
  val_batch_size: 256
  async_grpo:
    enabled: false
    max_trajectory_age_steps: 1

loss_fn:
  reference_policy_kl_penalty: 0.01
  ratio_clip_min: 0.2
  ratio_clip_max: 0.2
  # (default off) loss formulation improvements (docs/guides/grpo.md#loss)
  use_on_policy_kl_approximation: false
  use_importance_sampling_correction: false
  token_level_loss: true
  ratio_clip_c: null

checkpointing:
  enabled: false
  checkpoint_dir: "results/grpo_megatron"
  metric_name: "val:accuracy" # one of "val:" or "train:" followed by the metric name
  higher_is_better: true
  keep_top_k: 3
  save_period: 10
  checkpoint_must_save_by: null

policy:
  model_name: "Qwen/Qwen2.5-1.5B"
  tokenizer:
    name: ${policy.model_name} ## specify if you'd like to use a tokenizer different from the model's default
    chat_template_kwargs: null # can be used to pass kwargs to the chat template, e.g., enable_thinking=true
  train_global_batch_size: 512
  train_micro_batch_size: 4
  generation_batch_size: 64 # Only used when generating using megatron backend
  logprob_batch_size: ${policy.train_micro_batch_size}
  max_total_sequence_length: 512
  precision: "bfloat16"

  dtensor_cfg:
    enabled: false

  # See docs/design-docs/sequence-packing-and-dynamic-batching.md 
  # for more details on dynamic batching and sequence packing.
  #
  # We disable dynamic batching for Megatron as it is incompatible with Pipeline parallelism.
  # Instead, we use sequence packing.
  dynamic_batching:
    enabled: False
    train_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.train_micro_batch_size}}
    logprob_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.logprob_batch_size}}
    sequence_length_round: 64

  sequence_packing:
    enabled: True
    train_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.train_micro_batch_size}}
    logprob_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.logprob_batch_size}}
    algorithm: "modified_first_fit_decreasing"
    sequence_length_round: 64

  max_grad_norm: 1.0


  optimizer: null # remove default FSDP optimizer

  generation:
    backend: "vllm"
    max_new_tokens: ${policy.max_total_sequence_length}
    temperature: 1.0
    top_p: 1.0
    top_k: null
    mcore_generation_config:
      buffer_size_gb: 20  # Total GPU memory (in GB) allocated for KV cache buffers
      buffer_guaranteed_fraction: 0.1  # Fraction of buffer reserved for guaranteed active requests
      num_cuda_graphs: 16  # Number of CUDA graphs to pre-compile for different batch sizes
      block_size_tokens: 256  # Size of each KV cache block in tokens (affects memory granularity)
      use_cuda_graphs_for_non_decode_steps: true  # Enable CUDA graphs for prefill/context processing
      enable_chunked_prefill: true  # Split long prefills into chunks for better memory management
      unified_memory_level: 0  # Unified memory usage level (0=disabled, higher values enable more aggressive paging)
      max_tokens: 16384 # Maximum number of tokens to use in a single step. Analogous to vllm's max_num_batched_tokens
      
    vllm_cfg:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.6
      max_model_len: ${policy.max_total_sequence_length}

env:
  math:
    num_workers: 8
    math_verify_impl: "hf_math_verify"

logger:
  log_dir: "logs"  # Base directory for all logs
  num_val_samples_to_print: 0 # Number of validation samples to pretty print on terminal
  wandb_enabled: false
  tensorboard_enabled: false
  mlflow_enabled: false  # Disable MLflow logging
  swanlab_enabled: false # Disable SwanLab logging
  monitor_gpus: false  # If true, will monitor GPU usage and log to wandb and/or tensorboard
  wandb:
    project: "grpo-dev"
    name: "sj_megatron_1B"
  swanlab:
    project: "grpo-dev"
    name: "sj_megatron_1B"
  tensorboard: {}
  mlflow:
    experiment_name: "grpo-dev"
    run_name: "sj_megatron_1B"
  gpu_monitoring:
    collection_interval: 10  # How often to collect GPU usage metrics (in seconds)
    flush_interval: 10  # How often to flush GPU usage metrics to the loggers (in seconds)

cluster:
  gpus_per_node: 1
  num_nodes: 1
