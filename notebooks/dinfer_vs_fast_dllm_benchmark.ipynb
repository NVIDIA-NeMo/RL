{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dInfer vs Fast-dLLM Benchmarking\n",
    "\n",
    "This notebook benchmarks **dInfer** generation methods against **Fast-dLLM dual-cache** method.\n",
    "\n",
    "## Overview\n",
    "\n",
    "**dInfer Methods:**\n",
    "- `dinfer_blockwise`: BlockWise with threshold decoder (recommended)\n",
    "- `dinfer_hierarchy`: BlockWise with hierarchical decoder\n",
    "- `dinfer_credit`: BlockWise with credit threshold decoder\n",
    "\n",
    "**Fast-dLLM Baseline:**\n",
    "- `dual_cache`: Dual caching for optimal performance\n",
    "\n",
    "**Comparison Metrics:**\n",
    "- Tokens per second (throughput)\n",
    "- Average generation time\n",
    "- Number of forward evaluations (NFE)\n",
    "- Performance scaling across different generation lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPU Memory Cleanup\n",
    "\n",
    "Run this first to ensure a clean state before benchmarking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear all GPU memory and run garbage collection\"\"\"\n",
    "    print(\"üßπ Clearing GPU memory...\")\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Get GPU memory info\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "            allocated = torch.cuda.memory_allocated(i) / 1024**3\n",
    "            cached = torch.cuda.memory_reserved(i) / 1024**3\n",
    "            print(f\"   GPU {i}: {allocated:.2f}GB allocated, {cached:.2f}GB cached of {gpu_memory:.2f}GB total\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"   ‚úÖ CUDA cache cleared\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  CUDA not available\")\n",
    "    \n",
    "    # Get system memory info\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"   üíæ System RAM: {memory.used / 1024**3:.2f}GB used of {memory.total / 1024**3:.2f}GB total\")\n",
    "    print(\"   üóëÔ∏è  Garbage collection completed\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Clear memory at startup\n",
    "clear_gpu_memory()\n",
    "\n",
    "# Optional: Set memory management flags\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "    print(\"üîß PyTorch CUDA memory management configured\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries and Setup Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "import gc\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"GSAI-ML/LLaDA-8B-Instruct\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Project Discovery and Import Generation Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_project_root(start_path=None, max_depth=10):\n",
    "    \"\"\"Dynamically find the NeMo-RL project root.\"\"\"\n",
    "    if start_path is None:\n",
    "        start_path = os.getcwd()\n",
    "    \n",
    "    project_markers = ['pyproject.toml', 'notebooks', 'xp', 'nemo_rl', '3rdparty']\n",
    "    current_dir = os.path.abspath(start_path)\n",
    "    depth = 0\n",
    "    \n",
    "    print(f\"üîç Looking for project root starting from: {current_dir}\")\n",
    "    \n",
    "    while depth < max_depth:\n",
    "        print(f\"  Checking: {current_dir}\")\n",
    "        \n",
    "        try:\n",
    "            dir_contents = os.listdir(current_dir)\n",
    "            found_markers = [marker for marker in project_markers if marker in dir_contents]\n",
    "            print(f\"    Found markers: {found_markers}\")\n",
    "            \n",
    "            if len(found_markers) >= 3:\n",
    "                print(f\"  ‚úÖ Found project root: {current_dir}\")\n",
    "                return current_dir\n",
    "                \n",
    "        except PermissionError:\n",
    "            print(f\"    ‚ùå Permission denied\")\n",
    "        except Exception as e:\n",
    "            print(f\"    ‚ùå Error: {e}\")\n",
    "        \n",
    "        parent_dir = os.path.dirname(current_dir)\n",
    "        if parent_dir == current_dir:\n",
    "            break\n",
    "        current_dir = parent_dir\n",
    "        depth += 1\n",
    "    \n",
    "    print(f\"  ‚ùå Could not find project root within {max_depth} levels\")\n",
    "    return None\n",
    "\n",
    "def find_llada_generate_path(project_root):\n",
    "    \"\"\"Find the llada_generate module path.\"\"\"\n",
    "    if not project_root:\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüîç Searching for llada_generate module...\")\n",
    "    \n",
    "    llada_generate_locations = ['xp/llada_api', 'llada_api', 'api', 'src']\n",
    "    \n",
    "    for location in llada_generate_locations:\n",
    "        candidate_path = os.path.join(project_root, location)\n",
    "        llada_generate_path = os.path.join(candidate_path, 'llada_generate')\n",
    "        \n",
    "        print(f\"  Checking: {llada_generate_path}\")\n",
    "        \n",
    "        if os.path.exists(llada_generate_path):\n",
    "            init_file = os.path.join(llada_generate_path, '__init__.py')\n",
    "            if os.path.exists(init_file):\n",
    "                print(f\"  ‚úÖ Found llada_generate module!\")\n",
    "                return candidate_path\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  Directory exists but missing __init__.py\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Not found\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Discover project root and add llada_generate to path\n",
    "print(\"üöÄ Starting dynamic project discovery...\")\n",
    "project_root = find_project_root()\n",
    "\n",
    "GENERATION_AVAILABLE = False\n",
    "if project_root:\n",
    "    llada_generate_parent_path = find_llada_generate_path(project_root)\n",
    "    \n",
    "    if llada_generate_parent_path:\n",
    "        if llada_generate_parent_path not in sys.path:\n",
    "            sys.path.insert(0, llada_generate_parent_path)\n",
    "            print(f\"‚úÖ Added llada_generate parent path: {llada_generate_parent_path}\")\n",
    "        \n",
    "        try:\n",
    "            from llada_generate import (\n",
    "                get_algorithm,\n",
    "                list_available_algorithms,\n",
    "                list_algorithms,\n",
    "                list_algorithms_by_engine,\n",
    "                list_available_algorithms_by_engine,\n",
    "                registry\n",
    "            )\n",
    "            \n",
    "            GENERATION_AVAILABLE = True\n",
    "            print(\"‚úÖ Generation registry imported successfully!\")\n",
    "            \n",
    "            # List all algorithms\n",
    "            all_algorithms = list_algorithms()\n",
    "            available_algorithms = list_available_algorithms()\n",
    "            \n",
    "            print(f\"\\nüìã Total registered algorithms: {len(all_algorithms)}\")\n",
    "            for name in all_algorithms:\n",
    "                status = \"‚úÖ Available\" if name in available_algorithms else \"‚ùå Not available\"\n",
    "                algo_info = registry.get_algorithm_info(name)\n",
    "                if algo_info:\n",
    "                    print(f\"   {name}: {status} - {algo_info.get('description', 'No description')}\")\n",
    "                else:\n",
    "                    print(f\"   {name}: {status}\")\n",
    "            \n",
    "            # Show algorithms by engine\n",
    "            print(f\"\\nüîß Algorithms by engine:\")\n",
    "            for engine in ['dinfer', 'fast-dllm']:\n",
    "                engine_algos = list_available_algorithms_by_engine(engine)\n",
    "                print(f\"   {engine}: {engine_algos}\")\n",
    "            \n",
    "        except ImportError as e:\n",
    "            GENERATION_AVAILABLE = False\n",
    "            print(f\"‚ùå Failed to import generation registry: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Could not find llada_generate module\")\n",
    "else:\n",
    "    print(f\"‚ùå Could not locate project root\")\n",
    "\n",
    "if not GENERATION_AVAILABLE:\n",
    "    raise RuntimeError(\"Generation registry is not available. Cannot proceed with benchmarking.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Benchmarking Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking parameters\n",
    "BENCHMARK_CONFIG = {\n",
    "    'prompt_text': \"Explain the theory of general relativity in comprehensive detail.\",\n",
    "    'generation_lengths': [256, 512, 1024, 2048],  # Token lengths to benchmark\n",
    "    'num_trials': 3,  # Number of trials per benchmark for averaging\n",
    "    'show_generated_text': True,  # Whether to display sample generated text\n",
    "    'block_length': 64,  # Block length for generation (used by both dInfer and Fast-dLLM)\n",
    "}\n",
    "\n",
    "# Algorithms to benchmark\n",
    "ALGORITHMS_TO_BENCHMARK = [\n",
    "    # dInfer algorithms\n",
    "    'dinfer_blockwise',\n",
    "    'dinfer_hierarchy',\n",
    "    'dinfer_credit',\n",
    "    # Fast-dLLM baseline\n",
    "    'dual_cache',\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Benchmarking parameters configured:\")\n",
    "print(f\"   Prompt: {BENCHMARK_CONFIG['prompt_text'][:50]}...\")\n",
    "print(f\"   Generation lengths: {BENCHMARK_CONFIG['generation_lengths']}\")\n",
    "print(f\"   Trials per length: {BENCHMARK_CONFIG['num_trials']}\")\n",
    "print(f\"   Block length: {BENCHMARK_CONFIG['block_length']}\")\n",
    "print(f\"   Algorithms to test: {ALGORITHMS_TO_BENCHMARK}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Benchmarking Framework\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(\n",
    "    algorithm_name,\n",
    "    algorithm,\n",
    "    prompt_text,\n",
    "    gen_length,\n",
    "    block_length,\n",
    "    num_trials=3,\n",
    "    print_text=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Run benchmark for a specific algorithm.\n",
    "    \n",
    "    Args:\n",
    "        algorithm_name: Name of the algorithm\n",
    "        algorithm: Algorithm instance\n",
    "        prompt_text: Prompt string\n",
    "        gen_length: Generation length\n",
    "        block_length: Block length\n",
    "        num_trials: Number of trials\n",
    "        print_text: Whether to print generated text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with benchmark results\n",
    "    \"\"\"\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    generated_texts = []\n",
    "    forward_passes = []\n",
    "    \n",
    "    # Prepare prompt\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "    \n",
    "    # Check if algorithm uses dInfer (requires attention_mask)\n",
    "    is_dinfer = algorithm.engine == 'dinfer'\n",
    "    \n",
    "    if is_dinfer:\n",
    "        # dInfer: tokenize with left-padding and attention_mask\n",
    "        input_ids, attention_mask = algorithm.tokenize_prompts_dinfer(\n",
    "            prompts=[prompt_text],\n",
    "            apply_chat_template=True,\n",
    "            messages=[messages]\n",
    "        )\n",
    "    else:\n",
    "        # Fast-dLLM: tokenize normally\n",
    "        input_ids = algorithm.tokenize_prompts(\n",
    "            prompts=[prompt_text],\n",
    "            apply_chat_template=True,\n",
    "            messages=[messages]\n",
    "        )\n",
    "    \n",
    "    # Warm-up run\n",
    "    print(f\"  Warm-up run for {gen_length} tokens...\")\n",
    "    try:\n",
    "        _ = algorithm.generate(\n",
    "            model=algorithm.model,\n",
    "            prompt=input_ids,\n",
    "            steps=128,  # Not used by dInfer but kept for compatibility\n",
    "            gen_length=gen_length,\n",
    "            block_length=block_length,\n",
    "            temperature=1.0,\n",
    "            threshold=0.9\n",
    "        )\n",
    "        torch.cuda.synchronize()\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è Warm-up failed: {e}\")\n",
    "    \n",
    "    # Timed trials\n",
    "    for i in range(num_trials):\n",
    "        print(f\"  Trial {i+1}/{num_trials} for {gen_length} tokens...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            output_ids, nfe = algorithm.generate(\n",
    "                model=algorithm.model,\n",
    "                prompt=input_ids,\n",
    "                steps=128,\n",
    "                gen_length=gen_length,\n",
    "                block_length=block_length,\n",
    "                temperature=1.0,\n",
    "                threshold=0.9\n",
    "            )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            \n",
    "            # Decode output\n",
    "            if is_dinfer:\n",
    "                generated_texts_batch = algorithm.decode_outputs_dinfer(\n",
    "                    output_ids, input_ids, skip_special_tokens=True\n",
    "                )\n",
    "                generated_text = generated_texts_batch[0]\n",
    "            else:\n",
    "                generated_text = algorithm.tokenizer.batch_decode(\n",
    "                    output_ids[:, input_ids.shape[1]:],\n",
    "                    skip_special_tokens=True\n",
    "                )[0]\n",
    "            \n",
    "            elapsed_time = end_time - start_time\n",
    "            actual_tokens = len(algorithm.tokenizer.encode(generated_text))\n",
    "            \n",
    "            # Use requested gen_length for fair comparison\n",
    "            num_tokens = gen_length\n",
    "            \n",
    "            total_time += elapsed_time\n",
    "            total_tokens += num_tokens\n",
    "            generated_texts.append(generated_text)\n",
    "            forward_passes.append(nfe)\n",
    "            \n",
    "            # Print generated text if requested and first trial\n",
    "            if print_text and i == 0:\n",
    "                print(f\"    üìù Generated Text (Trial {i+1}):\")\n",
    "                print(f\"    {'-' * 50}\")\n",
    "                preview = generated_text[:300] + ('...' if len(generated_text) > 300 else '')\n",
    "                for line in preview.split('\\n'):\n",
    "                    print(f\"    {line}\")\n",
    "                print(f\"    {'-' * 50}\")\n",
    "                nfe_info = f\", NFE: {nfe}\" if nfe is not None and nfe > 0 else \"\"\n",
    "                print(f\"    Requested: {gen_length} tokens, Generated: {actual_tokens} tokens, Time: {elapsed_time:.2f}s{nfe_info}\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Trial {i+1} failed: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            generated_text = f\"[ERROR: {e}]\"\n",
    "            generated_texts.append(generated_text)\n",
    "            forward_passes.append(None)\n",
    "    \n",
    "    avg_time = total_time / num_trials if num_trials > 0 else 0\n",
    "    avg_tokens = total_tokens / num_trials if num_trials > 0 else 0\n",
    "    tokens_per_sec = avg_tokens / avg_time if avg_time > 0 else 0\n",
    "    avg_nfe = sum(nfe for nfe in forward_passes if nfe is not None and nfe > 0) / len([nfe for nfe in forward_passes if nfe is not None and nfe > 0]) if any(nfe is not None and nfe > 0 for nfe in forward_passes) else None\n",
    "    \n",
    "    result = {\n",
    "        \"Algorithm\": algorithm_name,\n",
    "        \"Engine\": algorithm.engine,\n",
    "        \"Gen Length\": gen_length,\n",
    "        \"Avg Time (s)\": avg_time,\n",
    "        \"Avg Tokens\": avg_tokens,\n",
    "        \"Tokens/Sec\": tokens_per_sec,\n",
    "        \"Avg NFE\": avg_nfe\n",
    "    }\n",
    "    \n",
    "    if print_text:\n",
    "        result[\"Generated_Texts\"] = generated_texts\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"‚úÖ Benchmarking framework loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Models and Run Benchmarks\n",
    "\n",
    "This cell will iterate through each algorithm, load its model, and run the benchmarks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Filter to only available algorithms\n",
    "available_algos = list_available_algorithms()\n",
    "algorithms_to_test = [name for name in ALGORITHMS_TO_BENCHMARK if name in available_algos]\n",
    "\n",
    "if not algorithms_to_test:\n",
    "    print(\"‚ùå No algorithms available for benchmarking!\")\n",
    "    print(f\"   Requested: {ALGORITHMS_TO_BENCHMARK}\")\n",
    "    print(f\"   Available: {available_algos}\")\n",
    "else:\n",
    "    print(f\"\\nüöÄ Starting benchmarking of {len(algorithms_to_test)} algorithms...\")\n",
    "    print(f\"   Algorithms: {algorithms_to_test}\")\n",
    "    print(f\"   Generation lengths: {BENCHMARK_CONFIG['generation_lengths']}\")\n",
    "    print(f\"   Trials per test: {BENCHMARK_CONFIG['num_trials']}\")\n",
    "    print(f\"   Block length: {BENCHMARK_CONFIG['block_length']}\")\n",
    "    \n",
    "    for algo_name in algorithms_to_test:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"üî¨ BENCHMARKING ALGORITHM: {algo_name.upper()}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Get algorithm instance\n",
    "        algorithm = get_algorithm(algo_name)\n",
    "        if algorithm is None:\n",
    "            print(f\"‚ùå Failed to get algorithm: {algo_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Display algorithm info\n",
    "        algo_info = registry.get_algorithm_info(algo_name)\n",
    "        if algo_info:\n",
    "            print(f\"üìã Description: {algo_info['description']}\")\n",
    "            print(f\"üìã Engine: {algo_info['engine']}\")\n",
    "        \n",
    "        # Load model\n",
    "        print(f\"\\nüì¶ Loading model: {MODEL_NAME}...\")\n",
    "        try:\n",
    "            success = algorithm.load_model_from_hf(\n",
    "                MODEL_NAME,\n",
    "                model_type='llada'\n",
    "            )\n",
    "            \n",
    "            if not success:\n",
    "                print(f\"‚ùå Failed to load model for {algo_name}\")\n",
    "                continue\n",
    "            \n",
    "            # Move model to GPU\n",
    "            algorithm.model = algorithm.model.to(DEVICE)\n",
    "            algorithm.model.eval()\n",
    "            algorithm.device = DEVICE\n",
    "            \n",
    "            print(f\"‚úÖ Model loaded successfully on {DEVICE}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "        \n",
    "        # Run benchmarks for different generation lengths\n",
    "        for i, gen_length in enumerate(BENCHMARK_CONFIG['generation_lengths']):\n",
    "            print(f\"\\nüìè Testing {algo_name} with {gen_length} tokens...\")\n",
    "            \n",
    "            # Show sample text only for first length\n",
    "            show_text = BENCHMARK_CONFIG['show_generated_text'] and (i == 0)\n",
    "            \n",
    "            try:\n",
    "                result = run_benchmark(\n",
    "                    algorithm_name=algo_name,\n",
    "                    algorithm=algorithm,\n",
    "                    prompt_text=BENCHMARK_CONFIG['prompt_text'],\n",
    "                    gen_length=gen_length,\n",
    "                    block_length=BENCHMARK_CONFIG['block_length'],\n",
    "                    num_trials=BENCHMARK_CONFIG['num_trials'],\n",
    "                    print_text=show_text\n",
    "                )\n",
    "                results_list.append(result)\n",
    "                \n",
    "                # Print performance summary\n",
    "                print(f\"   ‚ö° {algo_name}: {result['Tokens/Sec']:.2f} tokens/sec, {result['Avg Time (s)']:.2f}s avg\", end=\"\")\n",
    "                if result['Avg NFE'] is not None:\n",
    "                    print(f\", {result['Avg NFE']:.1f} avg NFE\")\n",
    "                else:\n",
    "                    print()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå Failed to benchmark {algo_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        \n",
    "        # Clean up model from memory\n",
    "        print(f\"\\nüßπ Clearing {algo_name} model from memory...\")\n",
    "        del algorithm.model\n",
    "        if hasattr(algorithm, 'diffusion_llm') and algorithm.diffusion_llm is not None:\n",
    "            del algorithm.diffusion_llm\n",
    "            algorithm.diffusion_llm = None\n",
    "        algorithm.model = None\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"‚úÖ Memory cleared\")\n",
    "\n",
    "print(f\"\\n\\nüéâ All benchmarking completed!\")\n",
    "print(f\"   Total experiments: {len(results_list)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_list:\n",
    "    print(\"‚ùå No results to display!\")\n",
    "else:\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\nüìä COMPREHENSIVE BENCHMARKING RESULTS\")\n",
    "    print(\"=\" * 90)\n",
    "    print(\"üìè Note: All algorithms use requested generation length for fair comparison\")\n",
    "    print()\n",
    "    \n",
    "    # Display results table\n",
    "    display_df = results_df.copy()\n",
    "    display_df = display_df.round(3)\n",
    "    print(display_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics by algorithm\n",
    "    print(\"\\nüìà PERFORMANCE SUMMARY BY ALGORITHM\")\n",
    "    print(\"=\" * 60)\n",
    "    summary = results_df.groupby('Algorithm').agg({\n",
    "        'Tokens/Sec': ['mean', 'max', 'min'],\n",
    "        'Avg Time (s)': ['mean', 'max', 'min'],\n",
    "        'Avg NFE': 'mean'\n",
    "    }).round(3)\n",
    "    print(summary)\n",
    "    \n",
    "    # Find best performing algorithms\n",
    "    print(\"\\nüèÜ TOP PERFORMERS\")\n",
    "    print(\"=\" * 40)\n",
    "    best_overall = results_df.loc[results_df['Tokens/Sec'].idxmax()]\n",
    "    print(f\"ü•á Fastest Overall: {best_overall['Algorithm']} at {best_overall['Gen Length']} tokens\")\n",
    "    print(f\"   Speed: {best_overall['Tokens/Sec']:.2f} tokens/sec\")\n",
    "    \n",
    "    # Best per generation length\n",
    "    for gen_length in BENCHMARK_CONFIG['generation_lengths']:\n",
    "        subset = results_df[results_df['Gen Length'] == gen_length]\n",
    "        if not subset.empty:\n",
    "            best = subset.loc[subset['Tokens/Sec'].idxmax()]\n",
    "            print(f\"   üìè {gen_length} tokens: {best['Algorithm']} ({best['Tokens/Sec']:.2f} tok/s)\")\n",
    "    \n",
    "    # Display the dataframe\n",
    "    display(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comprehensive Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_list:\n",
    "    # Set up plotting style\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    # Create comprehensive visualization\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Main performance comparison - Tokens per second\n",
    "    ax1 = plt.subplot(2, 3, (1, 2))\n",
    "    pivot_df = results_df.pivot(index='Gen Length', columns='Algorithm', values='Tokens/Sec')\n",
    "    pivot_df.plot(kind='bar', ax=ax1, width=0.8)\n",
    "    ax1.set_title('dInfer vs Fast-dLLM: Generation Speed Comparison', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Generation Length (tokens)', fontsize=12)\n",
    "    ax1.set_ylabel('Tokens per Second', fontsize=12)\n",
    "    ax1.tick_params(axis='x', rotation=0)\n",
    "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax1.legend(title='Algorithm', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for container in ax1.containers:\n",
    "        ax1.bar_label(container, fmt='%.1f', label_type='edge', fontsize=8, rotation=90)\n",
    "    \n",
    "    # 2. Average time comparison\n",
    "    ax2 = plt.subplot(2, 3, 3)\n",
    "    time_pivot = results_df.pivot(index='Gen Length', columns='Algorithm', values='Avg Time (s)')\n",
    "    time_pivot.plot(kind='bar', ax=ax2, width=0.8)\n",
    "    ax2.set_title('Average Time Comparison', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Generation Length (tokens)', fontsize=10)\n",
    "    ax2.set_ylabel('Time (seconds)', fontsize=10)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    ax2.legend().set_visible(False)\n",
    "    \n",
    "    # 3. Performance scaling\n",
    "    ax3 = plt.subplot(2, 3, 4)\n",
    "    for algorithm in results_df['Algorithm'].unique():\n",
    "        algorithm_data = results_df[results_df['Algorithm'] == algorithm]\n",
    "        ax3.plot(algorithm_data['Gen Length'], algorithm_data['Tokens/Sec'], 'o-',\n",
    "                label=algorithm, linewidth=2, markersize=6)\n",
    "    ax3.set_title('Performance Scaling by Length', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xlabel('Generation Length (tokens)', fontsize=10)\n",
    "    ax3.set_ylabel('Tokens per Second', fontsize=10)\n",
    "    ax3.grid(True, alpha=0.7)\n",
    "    ax3.legend(fontsize=8)\n",
    "    \n",
    "    # 4. NFE comparison (if available)\n",
    "    ax4 = plt.subplot(2, 3, 5)\n",
    "    if 'Avg NFE' in results_df.columns and results_df['Avg NFE'].notna().any():\n",
    "        nfe_df = results_df[results_df['Avg NFE'].notna()]\n",
    "        if not nfe_df.empty:\n",
    "            nfe_pivot = nfe_df.pivot(index='Gen Length', columns='Algorithm', values='Avg NFE')\n",
    "            nfe_pivot.plot(kind='bar', ax=ax4, width=0.8)\n",
    "            ax4.set_title('Forward Evaluations (NFE)', fontsize=12, fontweight='bold')\n",
    "            ax4.set_xlabel('Generation Length (tokens)', fontsize=10)\n",
    "            ax4.set_ylabel('Number of Forward Evaluations', fontsize=10)\n",
    "            ax4.tick_params(axis='x', rotation=45)\n",
    "            ax4.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "            ax4.legend(fontsize=8)\n",
    "        else:\n",
    "            ax4.text(0.5, 0.5, 'No NFE data available', ha='center', va='center', transform=ax4.transAxes)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'No NFE data available', ha='center', va='center', transform=ax4.transAxes)\n",
    "    ax4.set_title('Forward Evaluations (NFE)', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # 5. Performance heatmap\n",
    "    ax5 = plt.subplot(2, 3, 6)\n",
    "    heatmap_data = results_df.pivot(index='Algorithm', columns='Gen Length', values='Tokens/Sec')\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax5.imshow(heatmap_data.values, cmap='YlOrRd', aspect='auto')\n",
    "    \n",
    "    # Set ticks and labels\n",
    "    ax5.set_xticks(range(len(heatmap_data.columns)))\n",
    "    ax5.set_yticks(range(len(heatmap_data.index)))\n",
    "    ax5.set_xticklabels(heatmap_data.columns)\n",
    "    ax5.set_yticklabels(heatmap_data.index)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(heatmap_data.index)):\n",
    "        for j in range(len(heatmap_data.columns)):\n",
    "            value = heatmap_data.iloc[i, j]\n",
    "            if not pd.isna(value):\n",
    "                ax5.text(j, i, f'{value:.1f}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax5)\n",
    "    cbar.set_label('Tokens/Sec', rotation=270, labelpad=15)\n",
    "    \n",
    "    ax5.set_title('Performance Heatmap', fontsize=12, fontweight='bold')\n",
    "    ax5.set_xlabel('Generation Length (tokens)', fontsize=10)\n",
    "    ax5.set_ylabel('Algorithm', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Visualization complete!\")\n",
    "else:\n",
    "    print(\"‚ùå No results to visualize!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Performance Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_list:\n",
    "    print(\"\\nüîç DETAILED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Speed comparison: dInfer vs Fast-dLLM dual_cache\n",
    "    fast_dllm_results = results_df[results_df['Algorithm'] == 'dual_cache']\n",
    "    dinfer_results = results_df[results_df['Engine'] == 'dinfer']\n",
    "    \n",
    "    if not fast_dllm_results.empty and not dinfer_results.empty:\n",
    "        print(\"\\n‚ö° SPEED ANALYSIS: dInfer vs Fast-dLLM dual_cache\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for gen_length in BENCHMARK_CONFIG['generation_lengths']:\n",
    "            fast_dllm_speed = fast_dllm_results[fast_dllm_results['Gen Length'] == gen_length]['Tokens/Sec']\n",
    "            if len(fast_dllm_speed) > 0:\n",
    "                fast_dllm_speed = fast_dllm_speed.iloc[0]\n",
    "                \n",
    "                dinfer_subset = dinfer_results[dinfer_results['Gen Length'] == gen_length]\n",
    "                \n",
    "                print(f\"\\nüìè {gen_length} tokens (Fast-dLLM baseline: {fast_dllm_speed:.2f} tok/s):\")\n",
    "                \n",
    "                for _, row in dinfer_subset.iterrows():\n",
    "                    algo_name = row['Algorithm']\n",
    "                    speed_ratio = row['Tokens/Sec'] / fast_dllm_speed if fast_dllm_speed > 0 else 0\n",
    "                    \n",
    "                    if speed_ratio > 1.2:\n",
    "                        status = f\"üü¢ {speed_ratio:.2f}x FASTER\"\n",
    "                    elif speed_ratio > 0.8:\n",
    "                        status = f\"üü° {speed_ratio:.2f}x (comparable)\"\n",
    "                    else:\n",
    "                        status = f\"üî¥ {speed_ratio:.2f}x slower\"\n",
    "                    \n",
    "                    print(f\"   {algo_name:20s}: {row['Tokens/Sec']:6.2f} tok/s | {status}\")\n",
    "    \n",
    "    # Efficiency analysis\n",
    "    if 'Avg NFE' in results_df.columns and results_df['Avg NFE'].notna().any():\n",
    "        print(\"\\nüéØ EFFICIENCY ANALYSIS (Tokens per Forward Evaluation):\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for gen_length in BENCHMARK_CONFIG['generation_lengths']:\n",
    "            subset = results_df[(results_df['Gen Length'] == gen_length) & (results_df['Avg NFE'].notna())]\n",
    "            if not subset.empty:\n",
    "                print(f\"\\nüìè {gen_length} tokens:\")\n",
    "                \n",
    "                for _, row in subset.iterrows():\n",
    "                    if pd.notna(row['Avg NFE']) and row['Avg NFE'] > 0:\n",
    "                        efficiency = row['Avg Tokens'] / row['Avg NFE']\n",
    "                        print(f\"   {row['Algorithm']:20s}: {efficiency:6.2f} tokens/NFE ({row['Avg NFE']:5.1f} forward passes)\")\n",
    "    \n",
    "    # Key insights\n",
    "    print(\"\\nüí° KEY INSIGHTS:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    best_overall = results_df.loc[results_df['Tokens/Sec'].idxmax()]\n",
    "    worst_overall = results_df.loc[results_df['Tokens/Sec'].idxmin()]\n",
    "    \n",
    "    print(f\"‚Ä¢ Best performer: {best_overall['Algorithm']} ({best_overall['Tokens/Sec']:.2f} tok/s)\")\n",
    "    print(f\"‚Ä¢ Slowest performer: {worst_overall['Algorithm']} ({worst_overall['Tokens/Sec']:.2f} tok/s)\")\n",
    "    print(f\"‚Ä¢ Performance range: {best_overall['Tokens/Sec'] / worst_overall['Tokens/Sec']:.1f}x difference\")\n",
    "    \n",
    "    # Algorithm ranking by engine\n",
    "    for engine in ['dinfer', 'fast-dllm']:\n",
    "        engine_results = results_df[results_df['Engine'] == engine]\n",
    "        if not engine_results.empty:\n",
    "            engine_speeds = engine_results.groupby('Algorithm')['Tokens/Sec'].mean().sort_values(ascending=False)\n",
    "            print(f\"\\nüèÜ {engine.upper()} Algorithm Ranking (by average speed):\")\n",
    "            for i, (algo, speed) in enumerate(engine_speeds.items(), 1):\n",
    "                print(f\"   {i}. {algo}: {speed:.2f} tok/s\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Comprehensive benchmarking analysis complete!\")\n",
    "    print(f\"   üìä Total experiments: {len(results_df)}\")\n",
    "    print(f\"   üß† Algorithms tested: {len(results_df['Algorithm'].unique())}\")\n",
    "    print(f\"   üìà Generation lengths: {len(BENCHMARK_CONFIG['generation_lengths'])}\")\n",
    "    print(f\"   üîÑ Trials per config: {BENCHMARK_CONFIG['num_trials']}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to analyze!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results (Optional)\n",
    "\n",
    "Save the benchmark results to CSV for later analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_list:\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path(project_root) / 'notebooks' / 'benchmark_results'\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = output_dir / f'dinfer_vs_fastdllm_{timestamp}.csv'\n",
    "    \n",
    "    # Save to CSV\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "else:\n",
    "    print(\"‚ùå No results to export!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides a comprehensive benchmark comparison between dInfer and Fast-dLLM generation methods:\n",
    "\n",
    "### Algorithms Compared:\n",
    "1. **dInfer Methods** (using the new generation API):\n",
    "   - `dinfer_blockwise` - Threshold-based parallel decoder with dual cache\n",
    "   - `dinfer_hierarchy` - Hierarchical parallel decoder with dual cache\n",
    "   - `dinfer_credit` - Credit threshold parallel decoder with dual cache\n",
    "\n",
    "2. **Fast-dLLM Baseline**:\n",
    "   - `dual_cache` - Dual caching for optimal Fast-dLLM performance\n",
    "\n",
    "### Key Features:\n",
    "- ‚úÖ Uses the updated generation API from `llada_generate` module\n",
    "- ‚úÖ Automatic algorithm discovery and loading\n",
    "- ‚úÖ Multiple generation lengths tested (256, 512, 1024, 2048 tokens)\n",
    "- ‚úÖ Multiple trials per configuration for reliable averages\n",
    "- ‚úÖ Comprehensive visualizations and performance analysis\n",
    "- ‚úÖ Direct speed comparison with speedup ratios\n",
    "- ‚úÖ Efficiency analysis (tokens per forward evaluation)\n",
    "- ‚úÖ Results export to CSV\n",
    "\n",
    "### Usage Notes:\n",
    "- Ensure both dInfer and Fast-dLLM are installed in `3rdparty/` directory\n",
    "- The notebook automatically discovers the project root and imports the generation registry\n",
    "- Models are loaded and unloaded sequentially to manage GPU memory\n",
    "- Results can be exported to CSV for further analysis\n",
    "\n",
    "### Expected Output:\n",
    "- Performance tables showing tokens/sec for each algorithm\n",
    "- Visualization plots comparing speed, scaling, and efficiency\n",
    "- Detailed speed analysis showing which method is faster at each generation length\n",
    "- Algorithm rankings by engine type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dInfer vs Fast-dLLM Benchmarking\n",
    "\n",
    "This notebook benchmarks dInfer generation methods against Fast-dLLM dual-cache.\n",
    "\n",
    "**Comparison:**\n",
    "- **dInfer methods:** dinfer_blockwise, dinfer_hierarchy, dinfer_credit\n",
    "- **Fast-dLLM baseline:** dual_cache\n",
    "\n",
    "**Metrics:**\n",
    "- Generation speed (tokens/sec)\n",
    "- Average time per generation\n",
    "- Number of forward evaluations (NFE)\n",
    "- Performance scaling across different generation lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Memory Cleanup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
